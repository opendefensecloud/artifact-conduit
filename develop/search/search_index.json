{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#what-is-artifact-conduit-arc","title":"What is Artifact Conduit (ARC)?","text":"<p>ARC (Artifact Conduit) is an open-source system that acts as a gateway for procuring various artifact types and transferring them across security zones while ensuring policy compliance through automated scanning and validation. The system addresses the challenge of bringing external artifacts\u2014container images, Helm charts, software packages, and other resources\u2014into restricted environments where direct internet access is prohibited.</p> <p>Primary Goals:</p> <ul> <li>Artifact Procurement: Pull artifacts from diverse sources including OCI registries, Helm repositories, S3-compatible storage, and HTTP endpoints</li> <li>Security Validation: Perform malware scanning, CVE analysis, license verification, and signature validation before artifact transfer</li> <li>Policy Enforcement: Ensure only artifacts meeting defined security and compliance policies cross security boundaries</li> <li>Declarative Management: Leverage Kubernetes-native declarative configuration for artifact lifecycle management</li> <li>Auditability: Provide attestation and traceability of all artifact processing operations</li> </ul> <p>Out of Scope: ARC does not replace existing registry solutions or artifact repositories. It functions as an orchestration layer that coordinates artifact transfer and validation between existing infrastructure components.</p>"},{"location":"#system-architecture","title":"System Architecture","text":"<p>ARC is implemented as a Kubernetes Extension API Server integrated with the Kubernetes API Aggregation Layer. This architectural approach provides several advantages over Custom Resource Definitions (CRDs), including dedicated storage isolation, custom API implementation flexibility, and reduced risk to the hosting cluster's control plane.</p> <p></p> <p>Architecture: ARC System Components and Data Flow</p> <p>The system follows a layered architecture where users interact through the Kubernetes API, requests flow through the Kubernetes API aggregation layer to the ARC API Server, and the Order Controller orchestrates workflow execution by decomposing high-level Orders into executable ArtifactWorkflows.</p>"},{"location":"#core-concepts","title":"Core Concepts","text":"<p>ARC introduces four primary custom resource types under the <code>arc.bwi.de/v1alpha1</code> API group:</p> Resource Purpose Scope Order Declares intent to procure one or more artifacts with shared configuration defaults User-facing, high-level ArtifactWorkflow Represents a single artifact operation decomposed from an Order System-generated, execution unit Endpoint Defines a source or destination location with credentials Configuration, reusable ArtifactType Specifies processing rules and workflow templates for artifact types (e.g., \"oci\", \"helm\") Configuration, system-wide <pre><code>---\nconfig:\n  layout: elk\n---\nflowchart LR\n subgraph subGraph0[\"Declarative Layer\"]\n        Order[\"Order (User Input)\"]\n        Spec[\"spec:&lt;br&gt;- defaults&lt;br&gt;- artifacts[]&lt;br&gt;--- &lt;br&gt;status:&lt;br&gt;- endpointGenerations&lt;br&gt;- secretGenerations\"]\n  end\n subgraph subGraph1[\"Generated Layer\"]\n        ArtifactWorkflow1[\"ArtifactWorkflow-1&lt;br&gt;(Minimal Params)\"]\n        ArtifactWorkflow2[\"ArtifactWorkflow-2&lt;br&gt;(Minimal Params)\"]\n        ArtifactWorkflowN[\"ArtifactWorkflow-N&lt;br&gt;(Minimal Params)\"]\n  end\n subgraph Configuration[\"Configuration &amp; Secrets\"]\n        ArtifactTypeDef@{ label: \"ArtifactType (e.g., 'oci')\" }\n        EndpointSrc[\"Endpoint (Source)\"]\n        EndpointDst[\"Endpoint (Destination)\"]\n        EndpointSecret@{ shape: procs, label: \"Secrets (e.g. Credentials)\"}\n  end\n subgraph Execution[\"Execution\"]\n        WorkflowTemplate[\"WorkflowTemplate (Argo)\"]\n        WorkflowInstance1[\"Workflow Instance\"]\n        WorkflowInstance2[\"Workflow Instance\"]\n  end\n    Order -- contains --&gt; Spec\n    Spec -- generates --&gt; ArtifactWorkflow1 &amp; ArtifactWorkflow2 &amp; ArtifactWorkflowN\n    Spec -- reads &amp; tracks generations of --&gt; EndpointSrc &amp; EndpointDst &amp; EndpointSecret\n    ArtifactWorkflow1 -- type --&gt; ArtifactTypeDef\n    ArtifactWorkflow2 -- type --&gt; ArtifactTypeDef\n    ArtifactWorkflow1 -- srcRef --&gt; EndpointSrc\n    ArtifactWorkflow2 -- srcRef --&gt; EndpointSrc\n    ArtifactWorkflow1 -- dstRef --&gt; EndpointDst\n    ArtifactWorkflow2 -- dstRef --&gt; EndpointDst\n    EndpointSrc -- secretRef --&gt; EndpointSecret\n    EndpointDst -- secretRef --&gt; EndpointSecret\n    ArtifactTypeDef -- workflowTemplateRef --&gt; WorkflowTemplate\n    ArtifactWorkflow1 -- references --&gt; EndpointSecret\n    ArtifactWorkflow2 -- references --&gt; EndpointSecret\n    ArtifactWorkflow1 -- instantiates --&gt; WorkflowTemplate\n    ArtifactWorkflow2 -- instantiates --&gt; WorkflowTemplate\n    WorkflowTemplate -- instantiates --&gt; WorkflowInstance1 &amp; WorkflowInstance2\n    WorkflowInstance1 -- mounts --&gt; EndpointSecret\n    WorkflowInstance2 -- mounts --&gt; EndpointSecret\n\n    ArtifactTypeDef@{ shape: rect}</code></pre>  Hold \"Alt\" / \"Option\" to enable pan &amp; zoom"},{"location":"#key-components","title":"Key Components","text":""},{"location":"#arc-api-server","title":"ARC API Server","text":"<p>The ARC API Server is a Kubernetes Extension API Server implemented using the <code>k8s.io/apiserver</code> library. Key characteristics:</p> <ul> <li>Implementation Path: <code>pkg/apiserver/</code></li> <li>Storage Backend: Dedicated etcd instance (isolated from Kubernetes control plane etcd)</li> <li>Registry Pattern: Uses <code>pkg/registry/</code> for custom storage strategies per resource type</li> <li>API Group: <code>arc.bwi.de</code> with version <code>v1alpha1</code></li> <li>Integration: Registered with Kubernetes API Aggregation Layer to handle requests to <code>arc.bwi.de/*</code> paths</li> </ul> <p>The dedicated etcd approach provides:</p> <ul> <li>Isolation from the hosting cluster's control plane</li> <li>Flexibility to change storage backends if needed</li> <li>Protection against resource volume impacting cluster stability</li> </ul>"},{"location":"#order-controller","title":"Order Controller","text":"<p>The Order Controller implements the reconciliation loop for Order resources:</p> <ul> <li>Implementation Path: <code>pkg/controller/</code></li> <li>Framework: Uses <code>sigs.k8s.io/controller-runtime</code> (version 0.22.4)</li> <li> <p>Reconciliation Logic:</p> <ol> <li>Watch for Order create/update/delete events</li> <li>Validate endpoint references exist</li> <li>Apply defaults from Order.spec.defaults</li> <li>Generate ArtifactWorkflow resources (one per artifact entry)</li> <li>Lookup ArtifactType for each fragment's type</li> <li>Create Argo Workflow instances with appropriate WorkflowTemplate references</li> <li>Update Order status based on ArtifactWorkflow and Workflow statuses</li> <li>Handle finalizers for cleanup operations</li> </ol> </li> </ul> <pre><code>sequenceDiagram\n    participant User\n    participant K8sAPI as \"Kubernetes API\"\n    participant ARCAPI as \"ARC API Server&lt;br/&gt;pkg/apiserver/\"\n    participant etcd as \"Dedicated etcd\"\n    participant OrderCtrl as \"Order Controller&lt;br/&gt;pkg/controller/\"\n    participant ArgoCtrl as \"Argo Controller\"\n    participant Workflow as \"Workflow Pod\"\n    participant Registry as \"External Registry\"\n\n    K8sAPI-&gt;&gt;ARCAPI: \"Forward to arc.bwi.de\"\n    ARCAPI-&gt;&gt;etcd: \"Store Order\"\n\n    etcd--&gt;&gt;OrderCtrl: \"Watch notification\"\n    OrderCtrl-&gt;&gt;OrderCtrl: \"Reconcile()\"\n    OrderCtrl-&gt;&gt;ARCAPI: \"Create ArtifactWorkflow CRs\"\n    ARCAPI-&gt;&gt;etcd: \"Store ArtifactWorkflows\"\n\n    OrderCtrl-&gt;&gt;ARCAPI: \"Get ArtifactType\"\n    ARCAPI--&gt;&gt;OrderCtrl: \"ATD with workflowTemplateRef\"\n\n    OrderCtrl-&gt;&gt;K8sAPI: \"Create Workflow CR\"\n    K8sAPI--&gt;&gt;ArgoCtrl: \"Workflow created\"\n\n    ArgoCtrl-&gt;&gt;Workflow: \"Start workflow pods\"\n    Workflow-&gt;&gt;ARCAPI: \"Read Endpoint configs\"\n    ARCAPI--&gt;&gt;Workflow: \"Endpoint details + secrets\"\n\n    Workflow-&gt;&gt;Registry: \"Pull artifact\"\n    Registry--&gt;&gt;Workflow: \"Artifact data\"\n\n    Workflow-&gt;&gt;Workflow: \"Security scan\"\n    Workflow-&gt;&gt;Registry: \"Push to destination\"\n\n    Workflow--&gt;&gt;ArgoCtrl: \"Workflow complete\"\n    ArgoCtrl-&gt;&gt;K8sAPI: \"Update Workflow status\"\n\n    OrderCtrl-&gt;&gt;ARCAPI: \"Update Order status\"\n    ARCAPI-&gt;&gt;etcd: \"Store status update\"</code></pre>  Hold \"Alt\" / \"Option\" to enable pan &amp; zoom"},{"location":"CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#contributor-covenant-code-of-conduct","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address, without their explicit permission</li> <li>Contacting individual members, contributors, or leaders privately, outside designated community mechanisms, without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at opensource@github.com. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"CODE_OF_CONDUCT/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"CODE_OF_CONDUCT/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"CODE_OF_CONDUCT/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"CODE_OF_CONDUCT/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"CONTRIBUTING/","title":"Artifact Conduit (ARC) Developer Guide","text":""},{"location":"CONTRIBUTING/#artifact-conduit-arc-developer-guide","title":"Artifact Conduit (ARC) Developer Guide","text":"<p>This guide provides technical information for developers contributing to the Artifact Conduit (ARC) project. It covers the development workflow, build system, code organization, and common development tasks. For detailed information about specific topics, see the referenced sections.</p>"},{"location":"CONTRIBUTING/#development-workflow-overview","title":"Development Workflow Overview","text":"<p>ARC follows a code-generation-heavy pattern typical in Kubernetes ecosystem projects. Changes to API types trigger code regeneration, which produces client libraries, OpenAPI specifications, and CRD manifests.</p>"},{"location":"CONTRIBUTING/#build-system","title":"Build System","text":"<p>The ARC build system uses a Makefile to orchestrate various tools, designed for reproducibility. All required tools are provided in the <code>bin/</code> directory.</p> Target Purpose Key Tools Used <code>make codegen</code> Generate client-go libraries &amp; OpenAPI <code>openapi-gen</code>, <code>kube_codegen.sh</code> <code>make manifests</code> Generate CRDs and RBAC manifests <code>controller-gen</code> <code>make fmt</code> Format code, add license headers <code>addlicense</code>, <code>go fmt</code> <code>make lint</code> Run linters and checks <code>golangci-lint</code>, <code>shellcheck</code>, <code>addlicense</code> <code>make test</code> Run all tests with coverage <code>ginkgo</code>, <code>setup-envtest</code> <code>make clean</code> Remove generated binaries -"},{"location":"CONTRIBUTING/#tool-versions","title":"Tool Versions","text":"<p>The system pins specific tool versions for reproducibility:</p> <ul> <li>BDD testing framework: <code>v2.27.2</code></li> <li>Go linter: <code>v2.5.0</code></li> <li>CRD/RBAC generator: <code>v0.19.0</code></li> <li>Kubernetes test API server: <code>release-0.22</code></li> <li>K8s for integration tests: <code>1.34.1</code></li> </ul>"},{"location":"CONTRIBUTING/#codebase-organization","title":"Codebase Organization","text":"<p>ARC codebase follows standard Kubernetes project conventions:</p> Directory Purpose Generated/Manual <code>api/arc/v1alpha1/</code> Custom resource type definitions Manual <code>client-go/</code> Client libraries for ARC resources Generated <code>pkg/apiserver/</code> Extension API server implementation Manual <code>pkg/controller/</code> Controller reconciliation logic Manual <code>pkg/registry/</code> Storage strategies for custom resources Manual <code>config/</code> Kubernetes manifests (CRDs, RBAC) Generated <code>hack/</code> Build and code generation scripts Manual"},{"location":"CONTRIBUTING/#code-generation-process","title":"Code Generation Process","text":"<p>ARC uses the Kubernetes code-generator to produce client libraries and OpenAPI specs.</p> <ul> <li><code>make codegen</code> triggers <code>hack/update-codegen.sh</code></li> <li>Generates:</li> <li>Client-go libraries in <code>client-go/</code></li> <li>OpenAPI specs</li> <li>CRD manifests</li> </ul> <p>See Client Libraries section for usage details.</p>"},{"location":"CONTRIBUTING/#testing-strategy","title":"Testing Strategy","text":"<p>ARC uses a multi-layered testing strategy:</p> <ul> <li>Unit Tests</li> <li>Integration Tests (uses <code>ENVTEST_K8S_VERSION=1.34.1</code>)</li> <li>Controller Tests via envtest</li> </ul> <p>Run all tests and generate coverage:</p> <pre><code>make test\n</code></pre> <p>Setup environment for integration tests:</p> <pre><code>setup-envtest\nexport ENVTEST_K8S_VERSION=1.34.1\n</code></pre> <p>Test coverage is tracked using Coveralls.</p>"},{"location":"CONTRIBUTING/#continuous-integration-ci-pipeline","title":"Continuous Integration (CI) Pipeline","text":"<p>Pipeline runs on every push and pull request, enforcing code quality and test coverage.</p> <ul> <li>Lint Job</li> <li><code>addlicense</code></li> <li><code>shellcheck</code></li> <li><code>golangci-lint</code></li> <li>Test Job (runs after Lint)</li> <li><code>make test</code></li> </ul> <p>For customization details, see <code>.github/workflows/golang.yaml</code>.</p>"},{"location":"CONTRIBUTING/#adding-a-new-custom-resource","title":"Adding a New Custom Resource","text":"<p>To introduce a new CRD:</p> <ol> <li>Create type definition in <code>api/arc/v1alpha1/</code></li> <li>Add OpenAPI model name</li> <li>Regenerate code via <code>make codegen</code></li> <li>Implement storage in <code>pkg/registry/</code></li> <li>Add controller logic in <code>pkg/controller/</code> if reconciliation is needed</li> </ol> <p>See <code>hack/update-codegen.sh</code> for implementation details.</p>"},{"location":"CONTRIBUTING/#modifying-existing-api-types","title":"Modifying Existing API Types","text":"<p>Typical steps:</p> <ol> <li>Edit types in <code>api/arc/v1alpha1/</code></li> <li>Run <code>make codegen</code></li> <li>Run <code>make manifests</code></li> <li>Run <code>make test</code></li> </ol> <p>Note: Breaking changes may affect existing clients. Follow semantic versioning and provide migration paths.</p>"},{"location":"CONTRIBUTING/#code-quality-linting","title":"Code Quality &amp; Linting","text":"<p>Lint and license checks before committing:</p> <ul> <li><code>addlicense</code> for Apache 2.0 headers</li> <li><code>shellcheck</code> for scripts in <code>hack/</code></li> <li><code>golangci-lint</code> for Go linting</li> </ul> <p>Fix issues with:</p> <pre><code>make fmt\nmake lint\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#getting-started","title":"Getting started","text":"<p>Users interact with ARC primarily through the Kubernetes API.</p>"},{"location":"developer-guide/dod/","title":"Definition of Done","text":""},{"location":"developer-guide/dod/#definition-of-done","title":"Definition of Done","text":"<p>TODO</p>"},{"location":"developer-guide/working-with-docs/","title":"Documentation Setup","text":""},{"location":"developer-guide/working-with-docs/#documentation-setup","title":"Documentation Setup","text":"<p>The documentation of the ARC project is written primarily using Markdown. All documentation related content can be found in https://github.com/opendefensecloud/artifact-conduit/tree/main/docs. New content also should be added there.</p> <p>To render the documentation with <code>mkdocs</code> locally use <code>make docs</code> and open the local page:</p> <pre><code>$ make docs\nmkdocs serve\nINFO    -  Building documentation...\nINFO    -  Cleaning site directory\nINFO    -  Documentation built in 0.22 seconds\nINFO    -  [13:29:25] Watching paths for changes: 'docs', 'mkdocs.yml'\nINFO    -  [13:29:25] Serving on http://127.0.0.1:8000/\n</code></pre>"},{"location":"developer-guide/adrs/000-Use-Markdown-Architectural-Decision-Records/","title":"Find a Common Way to Document Architectural Design Decsisions","text":""},{"location":"developer-guide/adrs/000-Use-Markdown-Architectural-Decision-Records/#find-a-common-way-to-document-architectural-design-decsisions","title":"Find a Common Way to Document Architectural Design Decsisions","text":""},{"location":"developer-guide/adrs/000-Use-Markdown-Architectural-Decision-Records/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We want to record architectural decisions made in this project independent whether decisions concern the architecture (\"architectural decision record\"), the code, or other fields. Which format and structure should these records follow?</p>"},{"location":"developer-guide/adrs/000-Use-Markdown-Architectural-Decision-Records/#considered-options","title":"Considered Options","text":"<ul> <li>MADR\u00a04.0.0 \u2013 The Markdown Architectural Decision Records</li> <li>Michael Nygard's template\u00a0\u2013 The first incarnation of the term \"ADR\"</li> <li>Sustainable Architectural Decisions\u00a0\u2013 The Y-Statements</li> <li>Other templates listed at\u00a0https://github.com/joelparkerhenderson/architecture_decision_record</li> <li>Formless \u2013 No conventions for file format and structure</li> </ul>"},{"location":"developer-guide/adrs/000-Use-Markdown-Architectural-Decision-Records/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"MADR 4.0.0\", because</p> <ul> <li>Implicit assumptions should be made explicit. Design documentation is important to enable people understanding the decisions later on. See also\u00a0\"A rational design process: How and why to fake it\".</li> <li>MADR allows for structured capturing of any decision.</li> <li>The MADR format is lean and fits our development style.</li> <li>The MADR structure is comprehensible and facilitates usage &amp; maintenance.</li> <li>The MADR project is vivid.</li> </ul>"},{"location":"developer-guide/adrs/001-ARC-Architecture/","title":"Evaluate Future ARC Architecture Based on Predecessors","text":""},{"location":"developer-guide/adrs/001-ARC-Architecture/#evaluate-future-arc-architecture-based-on-predecessors","title":"Evaluate Future ARC Architecture Based on Predecessors","text":""},{"location":"developer-guide/adrs/001-ARC-Architecture/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>This ADR is about finding the right architecture for the ARC suite of services based on the knowledge gained during the internal predecessors of this project.</p>"},{"location":"developer-guide/adrs/001-ARC-Architecture/#glossary","title":"Glossary","text":"<ul> <li><code>Order</code>: Represents an order of one or more artifacts. Tracks generation numbers of referenced <code>Endpoints</code> and <code>Secrets</code> to detect changes and trigger <code>ArtifactWorkflow</code> reconciliation for idempotency.</li> <li><code>ArtifactWorkflow</code>: Represents a single artifact to be processed as part of an <code>Order</code>. Minimal resource containing artifact type reference and source/destination endpoint references. Replaces legacy <code>OrderArtifactWorkflow</code>.</li> <li><code>ArtifactType</code>: Defines rules, defaults, and the <code>WorkflowTemplate</code> reference for a specific artifact type like 'OCI'. Replaces legacy <code>OrderTypeDefinition</code>.</li> <li><code>Endpoint</code>: General term for source or destination. Can be a source or destination for artifacts. Includes optional credentials to access it. Changes to <code>Endpoint</code> or its referenced <code>Secret</code> trigger <code>Order</code> reconciliation.</li> <li><code>WorkflowTemplate</code>: Argo Workflows, see https://argo-workflows.readthedocs.io/en/latest/fields/#workflowtemplate</li> <li><code>Workflow</code>: Argo Workflows, see https://argo-workflows.readthedocs.io/en/latest/fields/#workflow</li> <li><code>ARC API Server</code>: A Kubernetes Extension API Server which handles storage of ARC API</li> <li><code>ARC Controller Manager</code>: A set of Kubernetes Controller which reconciles <code>Orders</code>, creates/updates <code>ArtifactWorkflow</code> resources from <code>Order</code> artifacts and instantiates <code>Workflow</code> resources for each <code>ArtifactWorkflow</code>. Tracks <code>Endpoint</code> and <code>Secret</code> generations to ensure idempotency.</li> </ul>"},{"location":"developer-guide/adrs/001-ARC-Architecture/#considered-options","title":"Considered Options","text":""},{"location":"developer-guide/adrs/001-ARC-Architecture/#classic-kubernetes-operators","title":"Classic Kubernetes Operators","text":"<ul> <li>CRDs are used to interact with ARC via the Kubernetes API Server.</li> <li>Several operators come into play which reconcile the different custom resources.</li> <li>A sharding mechanism is implemented to be able to scale the workers horizontally and give every worker a given chunk of resources to reconcile.</li> </ul>"},{"location":"developer-guide/adrs/001-ARC-Architecture/#pros","title":"Pros","text":"<ul> <li>CRDs and Kubernetes are relatively simple to implement</li> </ul>"},{"location":"developer-guide/adrs/001-ARC-Architecture/#cons","title":"Cons","text":"<ul> <li>Storage may be limited by <code>etcd</code> and can bring the control plane of Kubernetes into trouble if too many resources are present</li> <li>The necessity to implement sharding may be hard work and error prone</li> <li>Thus said it may not scale in way necessary for such a solution</li> </ul>"},{"location":"developer-guide/adrs/001-ARC-Architecture/#extension-api-server-and-cncf-landscape-tooling","title":"Extension API Server and CNCF Landscape Tooling","text":""},{"location":"developer-guide/adrs/001-ARC-Architecture/#flavor-a","title":"Flavor A","text":"<p>Flavor uses several controllers to handle different parts of the API. Orders are reconciled and converted to hydrated Orders which contain the source and destination information along with the artifact to process. This information are published to some message queue which is subscribed by workers working on that queue. Optionally KEDA is used to scale, handle quotas and fairness. The database to be used for the API Server is etcd.</p>"},{"location":"developer-guide/adrs/001-ARC-Architecture/#flavor-b","title":"Flavor B","text":"<p>Same as Flavor A except the database for the API Server is something like Postgres instead of etcd. Additionally the option is considered to let the controller access the Postgres directly to reconcile without using the Kubernetes API Server.</p>"},{"location":"developer-guide/adrs/001-ARC-Architecture/#flavor-c","title":"Flavor C","text":"<p>Same as Flavor B except that no message queue is used but the job queue is stored directly in the Postgres database.</p>"},{"location":"developer-guide/adrs/001-ARC-Architecture/#flavor-d","title":"Flavor D","text":"<p>Same as Flavor A except that no message queue is used. The Order controller creates hydrated orders directly in <code>etcd</code> as readonly resource which is then consumed by workers directly. This approach needs some kind of sharding mechanism to have the workers to know which shard of resources they need to handle.</p>"},{"location":"developer-guide/adrs/001-ARC-Architecture/#flavor-e","title":"Flavor E","text":"<p>This flavor is the most compelling due to the reduced amount of code that is necessary to bring this solution to live. etcd is used as storage for the API server. Argo Workflows is used to build workflows which do the steps necessary to process one artifact. Kueue can be used to bring fairness, scaling, quotas into play which workflows. The order controller creates \"jobs\" which are actually Argo Workflows.</p> <p>This option is described in detail in the following document.</p>"},{"location":"developer-guide/adrs/001-ARC-Architecture/#technology","title":"Technology","text":"<ul> <li>Instead of CRDs, <code>ARC</code> uses an Extension API Server via the Kubernetes API Aggregation Layer to handle API requests.</li> <li>This gives it the possibility to use a dedicated <code>etcd</code> or a even more suitable storage backend for the high amount of resources and status information in case this is necessary.</li> <li>While <code>etcd</code> still can be used as storage backend, it is one separated from the <code>etcd</code> used by the Kubernetes control plane and reduces the risk of bringing the whole cluster into trouble.</li> <li>As the storage implementation is under our control, we can implement a custom storage interface or use <code>kind</code> down the line.</li> <li>Additional links</li> <li>https://github.com/kubernetes-sigs/apiserver-runtime</li> <li>https://github.com/kubernetes/sample-apiserver/tree/master</li> <li>Utilize Argo Workflows to handle the workflows necessary to process different artifact types</li> <li>Optionally use Kueue to handle quotas and enhanced scheduling</li> <li>Namespaces are used to separate resources in a multi-tenant environment.</li> </ul>"},{"location":"developer-guide/adrs/001-ARC-Architecture/#architecture-diagram","title":"Architecture Diagram","text":"<p>Overview Diagram</p> <p></p> <p>API Concept Diagram</p> <pre><code>---\nconfig:\n  layout: elk\n---\nflowchart LR\n subgraph subGraph0[\"Declarative Layer\"]\n        Order[\"Order (User Input)\"]\n        Spec[\"spec:&lt;br&gt;- defaults&lt;br&gt;- artifacts[]&lt;br&gt;--- &lt;br&gt;status:&lt;br&gt;- endpointGenerations&lt;br&gt;- secretGenerations\"]\n  end\n subgraph subGraph1[\"Generated Layer\"]\n        ArtifactWorkflow1[\"ArtifactWorkflow-1&lt;br&gt;(Minimal Params)\"]\n        ArtifactWorkflow2[\"ArtifactWorkflow-2&lt;br&gt;(Minimal Params)\"]\n        ArtifactWorkflowN[\"ArtifactWorkflow-N&lt;br&gt;(Minimal Params)\"]\n  end\n subgraph Configuration[\"Configuration &amp; Secrets\"]\n        ArtifactTypeDef@{ label: \"ArtifactType (e.g., 'oci')\" }\n        EndpointSrc[\"Endpoint (Source)\"]\n        EndpointDst[\"Endpoint (Destination)\"]\n        EndpointSecret@{ shape: procs, label: \"Secrets (e.g. Credentials)\"}\n  end\n subgraph Execution[\"Execution\"]\n        WorkflowTemplate[\"WorkflowTemplate (Argo)\"]\n        WorkflowInstance1[\"Workflow Instance\"]\n        WorkflowInstance2[\"Workflow Instance\"]\n  end\n    Order -- contains --&gt; Spec\n    Spec -- generates --&gt; ArtifactWorkflow1 &amp; ArtifactWorkflow2 &amp; ArtifactWorkflowN\n    Spec -- reads &amp; tracks generations of --&gt; EndpointSrc &amp; EndpointDst &amp; EndpointSecret\n    ArtifactWorkflow1 -- type --&gt; ArtifactTypeDef\n    ArtifactWorkflow2 -- type --&gt; ArtifactTypeDef\n    ArtifactWorkflow1 -- srcRef --&gt; EndpointSrc\n    ArtifactWorkflow2 -- srcRef --&gt; EndpointSrc\n    ArtifactWorkflow1 -- dstRef --&gt; EndpointDst\n    ArtifactWorkflow2 -- dstRef --&gt; EndpointDst\n    EndpointSrc -- secretRef --&gt; EndpointSecret\n    EndpointDst -- secretRef --&gt; EndpointSecret\n    ArtifactTypeDef -- workflowTemplateRef --&gt; WorkflowTemplate\n    ArtifactWorkflow1 -- references --&gt; EndpointSecret\n    ArtifactWorkflow2 -- references --&gt; EndpointSecret\n    ArtifactWorkflow1 -- instantiates --&gt; WorkflowTemplate\n    ArtifactWorkflow2 -- instantiates --&gt; WorkflowTemplate\n    WorkflowTemplate -- instantiates --&gt; WorkflowInstance1 &amp; WorkflowInstance2\n    WorkflowInstance1 -- mounts --&gt; EndpointSecret\n    WorkflowInstance2 -- mounts --&gt; EndpointSecret\n\n    ArtifactTypeDef@{ shape: rect}</code></pre>  Hold \"Alt\" / \"Option\" to enable pan &amp; zoom  <p>The solution shows the ARC API Server which handles storage for the custom resources / API of ARC. <code>etcd</code> is used as storage solution. <code>Order Controller</code> is classic Kubernetes controller implementation which reconciles <code>Orders</code> and monitors <code>Endpoints</code> and <code>Secrets</code>. In a similar fashion the <code>ArtifactWorkflow Controller</code> reconciles <code>ArtifactWorkflows</code> and instantiates and tracks Argo Workflows based on the <code>ArtifactType</code>. Both controllers are contained in the <code>ARC Controller Manager</code>.</p> <p>An <code>Order</code> contains the information what artifacts should be processed and tracks generation numbers of referenced <code>Endpoints</code> and <code>Secrets</code> to detect changes. An <code>Endpoint</code> contains the information about a source or destination for artifacts. The <code>Order Controller</code> creates <code>ArtifactWorkflow</code> resources which are minimal, idempotent representations of artifacts to be processed. When an <code>Endpoint</code> or referenced <code>Secret</code> changes (detected via generation tracking), the controller automatically recreates the rendered configuration and triggers <code>ArtifactWorkflow</code> recreation. An <code>ArtifactType</code> specifies the processing rules and workflow templates for artifact types (e.g. <code>oci</code>, <code>helm</code>).</p>"},{"location":"developer-guide/adrs/001-ARC-Architecture/#pros_1","title":"Pros","text":"<ul> <li>Using dedicated <code>etcd</code> does not clutter the infra etcd</li> <li>Storage can be changed later on if necessary</li> <li>Keep the declarative style of Kubernetes while having complete freedom on the API implementation</li> <li>Argo Workflows allows us to focus on the domain of the product without reinventing the wheel</li> <li>Qotas and Fairness easy without writing code via Kueue</li> <li>Idempotency: Generation tracking ensures configuration changes are reliably detected and handled without race conditions</li> <li>Minimal Resource Overhead: Small <code>ArtifactWorkflow</code> resources with externalized configuration reduce API load and enable efficient scaling</li> </ul>"},{"location":"developer-guide/adrs/001-ARC-Architecture/#cons_1","title":"Cons","text":"<ul> <li>Building addon apiservers directly on the raw api-machinery libraries requires non-trivial code that must be maintained and rebased as the raw libraries change.</li> <li>Steep learning curve when starting the project and steeper learning curve when joining the project.</li> <li>Generation tracking adds complexity to the reconciliation logic that must be carefully maintained</li> </ul>"},{"location":"developer-guide/adrs/001-ARC-Architecture/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen Option: Solution E.</p> <p>Because the solution is the one that provides the most flexibility while the necessity to write own code for many parts is minimized. The flexibility comes from utilizing the CNCF projects Argo Workflows and Kueue for building the workflow engine. The project itself can focus on the order process and the handling of endpoints.</p>"},{"location":"developer-guide/adrs/002-ARC-API/","title":"Define an Optimal API for the Project Beginning","text":""},{"location":"developer-guide/adrs/002-ARC-API/#define-an-optimal-api-for-the-project-beginning","title":"Define an Optimal API for the Project Beginning","text":""},{"location":"developer-guide/adrs/002-ARC-API/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>This ADR is about finding the right API for ARC.</p>"},{"location":"developer-guide/adrs/002-ARC-API/#proposed-solution","title":"Proposed Solution","text":"<p>Options were discussed and documented here: https://app.bwi.conceptboard.com/board/u9c0-4nk5-rrhd-knre-6cfn</p>"},{"location":"developer-guide/adrs/002-ARC-API/#order","title":"<code>Order</code>","text":"<pre><code>apiVersion: arc.bwi.de/v1alpha1\nkind: Order\nmetadata:\n  name: example-order\nspec:\n  defaults:\n    srcRef:\n      name: docker-hub\n      namespace: default # optional\n    dstRef:\n      name: internal-registry\n  artifacts:\n    - type: oci # artifactType, correcesponds to workflow\n      dstRef:\n        name: other-internal-registry\n        namespace: default # optional\n      spec:\n        image: library/alpine:3.18\n        override: myteam/alpine:3.18-dev # default alpine:3.18; support CEL?\n    - type: oci\n      spec:\n        image: library/ubuntu:1.0\n    - type: helm\n      srcRef:\n        name: jetstack-helm\n      dstRef:\n        name: internal-helm-registry\n      spec:\n        name: cert-manager\n        version: \"47.11\"\n        override: helm-charts/cert-manager:47.11\n</code></pre>"},{"location":"developer-guide/adrs/002-ARC-API/#artifactworkflow","title":"<code>ArtifactWorkflow</code>","text":"<pre><code>apiVersion: arc.bwi.de/v1alpha1\nkind: ArtifactWorkflow\nmetadata:\n  name: example-order-1 # sha256 for procedural\nspec:\n  type: oci # artifactType, correcesponds to workflow\n  srcSecretRef:\n    name: lala\n  dstSecretRef:\n    name: other-internal-registry\n  parameters: # input from order used to hydrate parameters for workflow\n    - name: srcType\n      value: oci\n</code></pre>"},{"location":"developer-guide/adrs/002-ARC-API/#endpoint","title":"<code>Endpoint</code>","text":"<pre><code>apiVersion: arc.bwi.de/v1alpha1\nkind: Endpoint\nmetadata:\n  name: internal-registry\nspec:\n  type: oci # Endpoint Type! set valid types on controller manager?\n  remoteURL: https://artifactory.example.com/artifactory/ace-oci-local\n  secretRef: # STANDARDIZED!\n    name: internal-registry-credentials\n  usage: PullOnly | PushOnly | All # enum\n</code></pre>"},{"location":"developer-guide/adrs/002-ARC-API/#artifacttype","title":"<code>ArtifactType</code>","text":"<pre><code>apiVersion: arc.bwi.de/v1alpha1\nkind: ArtifactType\nmetadata:\n  name: oci\nspec:\n  rules:\n    srcTypes:\n      - s3 # Endpoint Types!\n      - oci\n      - helm\n    dstTypes:\n      - oci\n  workflowTemplateRef: # argo.Workflow\n</code></pre>"},{"location":"developer-guide/adrs/003-Branch-Naming/","title":"Name Branches Following a Common Standard","text":""},{"location":"developer-guide/adrs/003-Branch-Naming/#name-branches-following-a-common-standard","title":"Name Branches Following a Common Standard","text":""},{"location":"developer-guide/adrs/003-Branch-Naming/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>In our development using git as a versioned source control system, we're implementing our features, improvements, bugfixes, etc. in parallel branches before they get merged into the main branch. The names of these branches can lead to a misinterpretation of their intention. A naming scheme could help.</p>"},{"location":"developer-guide/adrs/003-Branch-Naming/#considered-solutions","title":"Considered Solutions","text":""},{"location":"developer-guide/adrs/003-Branch-Naming/#free-branch-naming","title":"Free Branch Naming","text":"<p>Branches can be named without any classification. The naming does not need any intention while a helpful name surely is allowed. Examples are:</p> <ul> <li><code>comment-adding</code></li> <li><code>readme</code></li> <li><code>evaluate-artifact-workflow</code></li> </ul>"},{"location":"developer-guide/adrs/003-Branch-Naming/#pros","title":"Pros","text":"<ul> <li>Quite simple, fast, and free</li> </ul>"},{"location":"developer-guide/adrs/003-Branch-Naming/#cons","title":"Cons","text":"<ul> <li>Not always helpful (see <code>readme</code>)</li> <li>Verbs describing the branch in different positions</li> <li>No grouping possible</li> </ul>"},{"location":"developer-guide/adrs/003-Branch-Naming/#detailed-prefixed-branch-names","title":"Detailed Prefixed Branch Names","text":"<p>Prefixes in fully written standard terms separated by a slash follow a well-known standard. These prefixes describe why a branch exists. Behind the slash, a small detail is used. Well-known prefixes as examples are:</p> <ul> <li><code>feature/short-description</code></li> <li><code>bugfix/issue-42</code></li> <li><code>hotfix/memory-overflow</code></li> <li><code>improvement/branch-naming</code></li> <li><code>experiment/iter-usage-in-looping</code></li> <li><code>release/2.1.0</code></li> </ul>"},{"location":"developer-guide/adrs/003-Branch-Naming/#pros_1","title":"Pros","text":"<ul> <li>Intention of a branch is clearer</li> <li>Grouping makes it easier to recognize parallel work</li> <li>Prefixes help to identify urgent tasks as opposed to regular tasks (<code>hotfix</code> opposite <code>experiment</code>)</li> </ul>"},{"location":"developer-guide/adrs/003-Branch-Naming/#cons_1","title":"Cons","text":"<ul> <li>Long branch names</li> <li>Intention has to be clear when branch is created</li> </ul>"},{"location":"developer-guide/adrs/003-Branch-Naming/#short-prefixed-branch-names","title":"Short Prefixed Branch Names","text":"<p>Similar to the detailed ones, but using abbreviations. Those could be <code>feat/</code>, <code>fix/</code>, <code>hot/</code>, <code>imp/</code>, <code>exp/</code>, or <code>rel/</code>.</p>"},{"location":"developer-guide/adrs/003-Branch-Naming/#pros_2","title":"Pros","text":"<ul> <li>Shorter names without losing the benefits of the detailed names</li> </ul>"},{"location":"developer-guide/adrs/003-Branch-Naming/#cons_2","title":"Cons","text":"<ul> <li>Same attention must be paid to the grouping</li> </ul>"},{"location":"developer-guide/adrs/003-Branch-Naming/#decision-outcome","title":"Decision Outcome","text":"<p>A comparison between versions two and three shows how much the use of prefixes helps in classifying branches. Although proposal 3 is more compact, proposal 2 is easier to recognise and is likely to lead to fewer errors in use.</p> <p>Initially, use of this standard will not be enforced technically, but will be treated as a convention. Later, local hooks will be offered that prevent developers from committing without a correctly named branch. We can offer this hook in a documented form, but it can be bypassed. Therefore, a GitHub Action can be used to prevent the merging of an incorrectly named branch.</p> <p>The branch prefixes and their meanings are:</p> <ul> <li><code>hotfix/</code> for hotfixe branches with very high priority</li> <li><code>bugfix/</code> for regular bug fixe branches</li> <li><code>feature/</code> for branches the which introduce new features</li> <li><code>improvement/</code> for branches doing refactorings and improvements to the code without introducing new features or API changes</li> <li><code>documentation/</code> for branches containing changes in the documentation and not in the code</li> <li><code>release/</code> for the final branches to be released</li> <li><code>evalaluation/</code> for the evaluation of new approaches and technologies; those typically will be dropped later and the results move into own issues and branches</li> </ul> <p>For all designations, the issue must be mentioned, for example <code>feat/176-etcd-garbage-cleanup</code>. Additional verbs like <code>176-add-etcd-garbage-cleanup</code> are not needed. For a release, however, the identifier for example is <code>rel/2.0.7</code>.</p>"},{"location":"operator-manual/installation/","title":"Installation","text":""},{"location":"operator-manual/installation/#installation","title":"Installation","text":"<p>TODO</p>"},{"location":"operator-manual/new-features/","title":"New Features","text":""},{"location":"operator-manual/new-features/#new-features","title":"New Features","text":"<p>TODO</p>"},{"location":"operator-manual/releases/","title":"Releases","text":""},{"location":"operator-manual/releases/#releases","title":"Releases","text":"<p>TODO</p>"},{"location":"operator-manual/security/","title":"Security","text":""},{"location":"operator-manual/security/#security","title":"Security","text":"<p>TODO</p>"},{"location":"operator-manual/upgrading/","title":"Upgrading","text":""},{"location":"operator-manual/upgrading/#upgrading","title":"Upgrading","text":"<p>TODO</p>"},{"location":"operator-manual/workflow-config/","title":"Workflow Config","text":""},{"location":"operator-manual/workflow-config/#workflow-config","title":"Workflow Config","text":"<p>ARC does not orchestrate the workflows, but relies on Argo Workflows as workflow engine.</p>"},{"location":"operator-manual/workflow-config/#resource-relationships","title":"Resource Relationships","text":"<p>The following diagram illustrates how ARC resources work together to instantiate and configure Argo Workflows:</p> <pre><code>graph TB\n    Order[\"\ud83d\udccb Order&lt;br/&gt;(User Request)\"]\n    ArtifactWorkflow[\"\ud83d\udce6 ArtifactWorkflow\"]\n    ArtifactTypeDef[\"\ud83c\udff7\ufe0f ArtifactType\"]\n    SrcEndpoint[\"\ud83d\udd0c Endpoint (Source)\"]\n    DstEndpoint[\"\ud83d\udd0c Endpoint (Destination)\"]\n    SrcSecret[\"\ud83d\udd10 Secret&lt;br/&gt;(Source Credentials)\"]\n    DstSecret[\"\ud83d\udd10 Secret&lt;br/&gt;(Destination Credentials)\"]\n    WorkflowTemplate[\"\u2699\ufe0f WorkflowTemplate\"]\n    Workflow[\"\ud83d\ude80 Workflow\"]\n\n    Order --&gt;|creates| ArtifactWorkflow\n    Order --&gt;|references| SrcEndpoint\n    Order --&gt;|references| DstEndpoint\n    ArtifactWorkflow --&gt;|specifies type| ArtifactTypeDef\n    ArtifactWorkflow --&gt;|references| SrcSecret\n    ArtifactWorkflow --&gt;|references| DstSecret\n\n    ArtifactTypeDef --&gt;|validates src/dst types| ArtifactWorkflow\n    ArtifactTypeDef --&gt;|references| WorkflowTemplate\n\n    SrcEndpoint --&gt;|references| SrcSecret\n    DstEndpoint --&gt;|references| DstSecret\n\n    WorkflowTemplate --&gt;|blueprint for| Workflow\n    ArtifactWorkflow --&gt;|provides params &amp; instantiates| Workflow\n    SrcSecret --&gt;|mounts to| Workflow\n    DstSecret --&gt;|mounts to| Workflow\n\n    style Order stroke:#e1f5ff,stroke-width:2px\n    style ArtifactWorkflow stroke:#f3e5f5,stroke-width:2px\n    style ArtifactTypeDef stroke:#e8f5e9,stroke-width:2px\n    style SrcEndpoint stroke:#fff3e0,stroke-width:2px\n    style DstEndpoint stroke:#fff3e0,stroke-width:2px\n    style SrcSecret stroke:#fce4ec,stroke-width:2px\n    style DstSecret stroke:#fce4ec,stroke-width:2px\n    style WorkflowTemplate stroke:#f1f8e9,stroke-width:2px\n    style Workflow stroke:#ffe0b2,stroke-width:2px</code></pre>  Hold \"Alt\" / \"Option\" to enable pan &amp; zoom"},{"location":"operator-manual/workflow-config/#walkthrough","title":"Walkthrough","text":"<p>A workflow created by ARC is composed out of three parts:</p> <ol> <li>A <code>workflowTemplateRef</code> which references a <code>WorkflowTemplate</code>-Object</li> <li>Parameters passed to the entrypoint of the workflow</li> <li>A mount for the source and destination secrets respectively</li> </ol> <p>When a <code>ArtifactWorkflow</code> is created (usually by an <code>Order</code> from a user) it might look as follows:</p> <pre><code>apiVersion: arc.bwi.de/v1alpha1\nkind: ArtifactWorkflow\nmetadata:\n  name: example-frag\nspec:\n  type: oci # Artifact Type!\n  srcSecretRef:\n    name: mysrc-creds\n  dstSecretRef:\n    name: mydst-creds\n  parameters:\n    - name: srcType\n      value: oci\n      # ...\n</code></pre> <p>The two referenced <code>Endpoints</code> by <code>srcRef</code> and <code>dstRef</code> might look as follows respectively:</p> <pre><code>apiVersion: arc.bwi.de/v1alpha1\nkind: Endpoint\nmetadata:\n  name: mysrc\nspec:\n  type: oci # Endpoint Type!\n  remoteURL: https://...\n  secretRef:\n    name: mysrc-creds\n  usage: PullOnly\n---\napiVersion: arc.bwi.de/v1alpha1\nkind: Endpoint\nmetadata:\n  name: mydst\nspec:\n  type: oci # Endpoint Type!\n  remoteURL: https://...\n  secretRef:\n    name: mydst-creds\n  usage: PushOnly\n</code></pre> <p>How these objects are tied into a workflow is described by the <code>ArtifactType</code>:</p> <pre><code>apiVersion: arc.bwi.de/v1alpha1\nkind: ArtifactType\nmetadata:\n  name: oci\nspec:\n  rules:\n    srcTypes: # Endpoint Types\n      - oci\n    dstTypes:\n      - oci\n  workflowTemplateRef: # argo.Workflow\n    name: oci-workflow-template\n</code></pre> <p>The <code>ArtifactWorkflow</code> defines which <code>ArtifactType</code> is used. In our case <code>oci</code> and therefore the controller will instantiate the <code>oci-workflow-template</code>.</p> <p>The two endpoints specified by the <code>ArtifactWorkflow</code> are compliant as the workflow does only support endpoints of the type <code>oci</code>. It is important to understand that there are both endpoint types and artifact types.</p> <p>The controller will verify the endpoints and retrieve the associated secrets.</p>"},{"location":"operator-manual/workflow-config/#resulting-parameters-and-runtime-configuration","title":"Resulting parameters and runtime-configuration","text":"<p>The above resources will instantiate the workflow with the following parameters:</p> <ul> <li><code>srcType</code>: <code>oci</code></li> <li><code>srcRemoteURL</code>: <code>https://...</code></li> <li><code>srcSecret</code>: <code>true</code> (special variable for conditional steps, <code>true</code> or <code>false</code> depending if secret was provided)</li> <li><code>dstType</code>: <code>oci</code></li> <li><code>dstRemoteURL</code>: <code>https://...</code></li> <li><code>dstSecret</code>: <code>true</code> (see above)</li> <li><code>specImage</code>: <code>library/alpine:3.18</code></li> <li><code>specOverride</code>: <code>myteam/alpine:3.18-dev</code></li> </ul> <p>Parameter names are derived from the API spec, but translated to camelCase. The values are always strings!</p> <p>The parameters do not contain secrets, but can be used to interact with third-party tools in the workflow and create conditional steps in the workflow, e.g. for different support source or destination types.</p> <p>However the source and destination secrets are mounted at <code>/secret/src/</code> and <code>/secret/dst/</code> respectively. If no secret was provided an emptyDir is mounted to make sure Argo Workflows continue to work.</p> <p>Using <code>oras</code> in a workflow might therefore look as follows:</p> <pre><code>oras pull -u \"$(cat /secret/src/username)\" -p \"$(cat /secret/src/password)\" {{ workflow.parameters.srcRemoteURL }}/{{ workflow.parameters.spec.image }}\n</code></pre>"},{"location":"operator-manual/workflow-config/#example-for-an-oci-usecase","title":"Example for an OCI usecase","text":""},{"location":"operator-manual/workflow-config/#workflowtemplate","title":"WorkflowTemplate","text":"<p>The following template is an example for a workflow that uses the <code>oci</code> source and destination. It can be used as a starting point to create your own workflows.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: WorkflowTemplate\nmetadata:\n  name: oci-image-pipeline-flexible\nspec:\n  # Define the PVC template for inter-step data sharing\n  volumeClaimTemplates:\n    - metadata:\n        name: work-volume\n      spec:\n        accessModes: [\"ReadWriteOnce\"]\n        resources:\n          requests:\n            storage: 10Gi\n\n  # --- Input Parameters ---\n  arguments:\n    parameters:\n      - name: srcType\n      - name: srcRemoteURL\n      - name: srcSecret\n      - name: dstType\n      - name: dstRemoteURL\n      - name: dstSecret\n      - name: specImage # The source image path (e.g., library/alpine:3.18)\n      - name: specOverride # The target image path (e.g., myteam/alpine:3.18-dev)\n      - name: scanSeverity\n        value: HIGH,CRITICAL\n\n  entrypoint: oci-pipeline\n\n  templates:\n    # --- Main Pipeline Entrypoint ---\n    - name: oci-pipeline\n      steps:\n        - - name: pull-image\n            when: \"{{workflow.parameters.srcType}} == oci\"\n            template: pull-image\n\n        - - name: scan-image\n            when: \"{{workflow.parameters.srcType}} == oci\"\n            template: scan-image\n\n        - - name: push-image\n            when: \"{{workflow.parameters.srcType}} == oci\"\n            template: push-image\n\n    # --- 1. Pull Image (Skopeo) ---\n    - name: pull-image\n\n      script:\n        image: quay.io/skopeo/stable:latest\n        command: [sh, -c]\n        # Mount the secret volume to a temporary directory\n        volumeMounts:\n          - name: src-secret-vol\n            readOnly: true\n            mountPath: /tmp/src-creds\n          - name: work-volume\n            mountPath: /data\n        source: |\n          echo \"Pulling image from {{workflow.parameters.srcRemoteURL}}/{{workflow.parameters.specImage}}\"\n          mkdir -p /data/image-storage\n\n          # Conditionally set credentials flag for Skopeo\n          CREDS_FLAG=\"\"\n          if [ \"{{workflow.parameters.srcSecret}}\" = \"true\" ]; then\n              # Skopeo uses the docker config JSON file (mounted from the secret)\n              CREDS_FLAG=\"--src-authfile /tmp/src-creds/.dockerconfigjson\"\n          fi\n\n          # Construct the full source image path\n          SOURCE_IMAGE=\"docker://{{workflow.parameters.srcRemoteURL}}/{{workflow.parameters.specImage}}\"\n\n          # Skopeo copy command\n          skopeo copy --multi-arch all $CREDS_FLAG $SOURCE_IMAGE \"oci:/data/image-storage/scanned-image\"\n\n    # --- 2. Scan Image (Trivy) ---\n    - name: scan-image\n\n      outputs:\n        artifacts:\n          - name: scan-result # name of output parameter\n            path: /data/scan-results.json\n\n      script:\n        image: aquasec/trivy:latest\n        command: [sh, -c]\n        volumeMounts:\n          - name: work-volume\n            mountPath: /data\n\n        source: |\n          echo \"Scanning image for severities: {{workflow.parameters.scanSeverity}}\"\n\n          # Trivy scan in a directory mode\n          trivy image \\\n            --exit-code 1 \\\n            --severity \"{{workflow.parameters.scanSeverity}}\" \\\n            --format json \\\n            --output /data/scan-results.json \\\n            --input /data/image-storage/scanned-image\n\n          if [ $? -eq 0 ]; then\n              echo \"\u2705 Scan successful and no vulnerabilities found above severity: {{workflow.parameters.scanSeverity}}\"\n          else\n              echo \"\u274c Scan failed!\"\n              exit 1\n          fi\n        env:\n          - name: PATH\n            value: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/share/trivy/sbin\n\n    # --- 3. Push Image (Skopeo) ---\n    - name: push-image\n\n      script:\n        image: quay.io/skopeo/stable:latest\n        command: [sh, -c]\n\n        # Mount the secret volume to a temporary directory\n        volumeMounts:\n          - name: dst-secret-vol\n            readOnly: true\n            mountPath: /tmp/dst-creds\n          - name: work-volume\n            mountPath: /data\n\n        source: |\n          echo \"Pushing image to {{workflow.parameters.dstRemoteURL}}/{{workflow.parameters.specOverride}}\"\n\n          # Conditionally set credentials flag for Skopeo\n          CREDS_FLAG=\"\"\n          if [ \"{{workflow.parameters.dstSecret}}\" = \"true\" ]; then\n              CREDS_FLAG=\"--dest-authfile /tmp/dst-creds/.dockerconfigjson\"\n          fi\n\n          # Construct the full destination image path\n          DEST_IMAGE=\"docker://{{workflow.parameters.dstRemoteURL}}/{{workflow.parameters.specOverride}}\"\n\n          # Skopeo copy command\n          skopeo copy --dest-tls-verify=false $CREDS_FLAG \"oci:/data/image-storage/scanned-image\" $DEST_IMAGE\n</code></pre>"},{"location":"operator-manual/workflow-config/#secrets","title":"Secrets","text":"<p>These are the example secrets for pulling and pushing.</p> <pre><code># Example Source Secret (src-reg-secret)\napiVersion: v1\nkind: Secret\nmetadata:\n  name: src-reg-secret\ntype: kubernetes.io/dockerconfigjson\ndata:\n  .dockerconfigjson: &lt;BASE64_ENCODED_AUTH_DATA_FOR_DOCKER.IO&gt;\n---\n# Example Destination Secret (dst-reg-secret)\napiVersion: v1\nkind: Secret\nmetadata:\n  name: dst-reg-secret\ntype: kubernetes.io/dockerconfigjson\ndata:\n  .dockerconfigjson: &lt;BASE64_ENCODED_AUTH_DATA_FOR_GHCR.IO&gt;\n</code></pre>"},{"location":"operator-manual/workflow-config/#workflow-example","title":"Workflow Example","text":"<p>To create a <code>Workflow</code> based on the template the following <code>yaml</code> can be used.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: oci-scan-flexible-\nspec:\n  # Reference the template defined above\n  workflowTemplateRef:\n    name: oci-image-pipeline-flexible\n\n  # --- Shared Volumes ---\n  volumes:\n    # This volume will contain the actual authentication file from the secret\n    - name: src-secret-vol\n      secret:\n        secretName: \"src-reg-secret\"\n    - name: dst-secret-vol\n      secret:\n        secretName: \"dst-reg-secret\"\n\n  # Pass the specific image paths and scan settings\n  arguments:\n    parameters:\n      # --- Source Parameters ---\n      - name: srcType\n        value: oci\n      - name: srcRemoteURL\n        value: docker.io\n      - name: srcSecret\n        value: \"false\"\n\n      # --- Destination Parameters ---\n      - name: dstType\n        value: oci\n      - name: dstRemoteURL\n        value: ghcr.io\n      - name: dstSecret\n        value: \"true\"\n\n      # --- Image Specs ---\n      - name: specImage\n        value: library/alpine:3.18\n      - name: specOverride\n        value: myteam/alpine:3.18-dev\n\n      # --- Scanning ---\n      - name: scanSeverity\n        value: HIGH,CRITICAL\n</code></pre>"},{"location":"user-guide/core-concepts/","title":"Core Concepts","text":""},{"location":"user-guide/core-concepts/#core-concepts","title":"Core Concepts","text":"<p>This document provides a comprehensive overview of the Artifact Conduit (ARC) system architecture, covering its design principles, core components, and how they interact.</p>"},{"location":"user-guide/core-concepts/#purpose-and-scope","title":"Purpose and Scope","text":"<p>ARC is a Kubernetes-native artifact management system designed to securely transport artifacts (OCI images, Helm charts, generic files) across network boundaries, particularly into air-gapped environments. The architecture employs a Kubernetes Extension API Server pattern to provide declarative resource management while maintaining flexibility for future storage backend changes.</p>"},{"location":"user-guide/core-concepts/#architectural-decision","title":"Architectural Decision","text":"<p>ARC's architecture is based on Architectural Decision Record 001, which selected an Extension API Server approach over traditional Custom Resource Definitions (CRDs). This design provides several key advantages:</p> Aspect Decision Rationale API Extension Kubernetes API Aggregation Layer Allows dedicated etcd instance, avoiding cluster control plane pollution Storage Backend Dedicated etcd (replaceable) Enables future migration to alternative storage if needed Workflow Engine Argo Workflows Leverages proven workflow orchestration without reinventing execution logic Declarative Model Kubernetes-style resources Maintains familiar kubectl/GitOps patterns for users <p>The Extension API Server pattern allows ARC to present a Kubernetes-native API surface while maintaining complete control over storage implementation and API behavior.</p>"},{"location":"user-guide/core-concepts/#system-components","title":"System Components","text":""},{"location":"user-guide/core-concepts/#component-overview","title":"Component Overview","text":"<pre><code>graph TB\n    subgraph \"Control Plane\"\n        K8sAPI[\"Kubernetes API Server\"]\n        APIAgg[\"API Aggregation Layer\"]\n    end\n\n    subgraph \"ARC System\"\n        APIServer[\"arc-apiserver&lt;br/&gt;(Extension API Server)\"]\n        etcd[\"Dedicated etcd&lt;br/&gt;(Storage Backend)\"]\n        CtrlMgr[\"arc-controller-manager\"]\n        OrderCtrl[\"OrderReconciler\"]\n    end\n\n    subgraph \"Custom Resources\"\n        Order[\"Order CR\"]\n        ArtifactWorkflow[\"ArtifactWorkflow CR\"]\n        Endpoint[\"Endpoint CR\"]\n        ATD[\"ArtifactType CR\"]\n    end\n\n    subgraph \"Execution Layer\"\n        Argo[\"Argo Workflows\"]\n        WorkflowTemplate[\"WorkflowTemplate\"]\n        Workflow[\"Workflow Instance\"]\n    end\n\n    subgraph \"External Systems\"\n        Registry[\"OCI Registries\"]\n        S3[\"S3 Storage\"]\n        Scanners[\"Security Scanners\"]\n    end\n\n    K8sAPI --&gt; APIAgg\n    APIAgg --&gt; APIServer\n    APIServer --&gt; etcd\n\n    CtrlMgr --&gt; OrderCtrl\n    OrderCtrl --&gt; K8sAPI\n    OrderCtrl --&gt; APIServer\n\n    Order --&gt; OrderCtrl\n    OrderCtrl --&gt; ArtifactWorkflow\n    OrderCtrl --&gt; Argo\n\n    ArtifactWorkflow --&gt; Endpoint\n    ArtifactWorkflow --&gt; ATD\n    ATD --&gt; WorkflowTemplate\n    Argo --&gt; Workflow\n\n    Workflow --&gt; Registry\n    Workflow --&gt; S3\n    Workflow --&gt; Scanners</code></pre>  Hold \"Alt\" / \"Option\" to enable pan &amp; zoom"},{"location":"user-guide/core-concepts/#arc-api-server","title":"ARC API Server","text":"<p>The API Server implements the Kubernetes Extension API Server pattern, registering with the Kubernetes API Aggregation Layer to handle requests for the <code>arc.bwi.de</code> API group.</p> <p>Key Characteristics:</p> Property Value Package <code>pkg/apiserver/</code> API Group <code>arc.bwi.de/v1alpha1</code> Storage Dedicated etcd cluster Registration Via APIService resource <p>The API Server is built using <code>apiserver-runtime</code> and <code>sample-apiserver</code> patterns, providing:</p> <ul> <li>Native Kubernetes authentication and authorization integration</li> <li>OpenAPI schema generation</li> <li>Support for standard Kubernetes API conventions (List, Watch, Get, Create, Update, Delete, Patch)</li> <li>Server-side apply functionality</li> </ul> <p>The test environment bootstraps the API Server programmatically for integration testing:</p> <pre><code>graph LR\n    TestEnv[\"envtest.Environment\"]\n    K8sControlPlane[\"envtest Control Plane\"]\n    APIServerProc[\"arc-apiserver Process\"]\n    APIService[\"APIService Resource\"]\n\n    TestEnv --&gt; K8sControlPlane\n    TestEnv --&gt; APIServerProc\n    APIServerProc --&gt; K8sControlPlane\n    K8sControlPlane --&gt; APIService</code></pre>  Hold \"Alt\" / \"Option\" to enable pan &amp; zoom"},{"location":"user-guide/core-concepts/#controller-manager","title":"Controller Manager","text":"<p>The controller manager binary (<code>arc-controller-manager</code>) hosts the Order controller and other reconciliation loops.</p> <p>Binary Location: cmd/arc-controller-manager/main.go:1-196</p> <p>Configuration:</p> Flag Default Purpose <code>--metrics-bind-address</code> <code>0</code> Metrics endpoint address <code>--health-probe-bind-address</code> <code>:8081</code> Health check endpoint <code>--leader-elect</code> <code>false</code> Enable leader election for HA <code>--metrics-secure</code> <code>true</code> Serve metrics over HTTPS <p>The manager initializes with:</p> <ul> <li>Kubernetes client-go scheme + ARC custom resources (cmd/arc-controller-manager/main.go:37-40)</li> <li>Controller-runtime manager with leader election support (cmd/arc-controller-manager/main.go:150-162)</li> <li>OrderReconciler registration (cmd/arc-controller-manager/main.go:173-178)</li> <li>Health and readiness checks (cmd/arc-controller-manager/main.go:182-189)</li> </ul>"},{"location":"user-guide/core-concepts/#resource-model","title":"Resource Model","text":""},{"location":"user-guide/core-concepts/#resource-hierarchy","title":"Resource Hierarchy","text":"<pre><code>graph TB\n    User[\"User/Operator\"]\n\n    subgraph \"Declarative Layer\"\n        Order[\"Order&lt;br/&gt;apiVersion: arc.bwi.de/v1alpha1&lt;br/&gt;kind: Order\"]\n        OrderSpec[\"spec:&lt;br/&gt;- defaults&lt;br/&gt;- artifacts[]\"]\n        OrderDefaults[\"defaults:&lt;br/&gt;- srcRef&lt;br/&gt;- dstRef\"]\n        OrderArtifacts[\"artifacts:&lt;br/&gt;- type&lt;br/&gt;- spec&lt;br/&gt;- srcRef/dstRef overrides\"]\n    end\n\n    subgraph \"Execution Layer\"\n        ArtifactWorkflow1[\"ArtifactWorkflow&lt;br/&gt;apiVersion: arc.bwi.de/v1alpha1&lt;br/&gt;kind: ArtifactWorkflow\"]\n        ArtifactWorkflow2[\"ArtifactWorkflow\"]\n        ArtifactWorkflowSpec[\"spec:&lt;br/&gt;- type&lt;br/&gt;- srcRef&lt;br/&gt;- dstRef&lt;br/&gt;- spec (type-specific)\"]\n    end\n\n    subgraph \"Configuration Layer\"\n        Endpoint1[\"Endpoint&lt;br/&gt;kind: Endpoint\"]\n        Endpoint2[\"Endpoint\"]\n        EndpointSpec[\"spec:&lt;br/&gt;- type&lt;br/&gt;- remoteURL&lt;br/&gt;- secretRef&lt;br/&gt;- usage\"]\n\n        ATD[\"ArtifactType&lt;br/&gt;kind: ArtifactType\"]\n        ATDSpec[\"spec:&lt;br/&gt;- rules&lt;br/&gt;- workflowTemplateRef\"]\n\n        Secret[\"Secret&lt;br/&gt;(credentials)\"]\n    end\n\n    User --&gt; Order\n    Order --&gt; OrderSpec\n    OrderSpec --&gt; OrderDefaults\n    OrderSpec --&gt; OrderArtifacts\n\n    OrderArtifacts -.generates.-&gt; ArtifactWorkflow1\n    OrderArtifacts -.generates.-&gt; ArtifactWorkflow2\n\n    ArtifactWorkflow1 --&gt; ArtifactWorkflowSpec\n    ArtifactWorkflowSpec --&gt; Endpoint1\n    ArtifactWorkflowSpec --&gt; Endpoint2\n    ArtifactWorkflowSpec --&gt; ATD\n\n    Endpoint1 --&gt; EndpointSpec\n    EndpointSpec --&gt; Secret\n    ATD --&gt; ATDSpec</code></pre>  Hold \"Alt\" / \"Option\" to enable pan &amp; zoom"},{"location":"user-guide/core-concepts/#resource-definitions","title":"Resource Definitions","text":""},{"location":"user-guide/core-concepts/#order","title":"Order","text":"<p>High-level declarative resource specifying one or more artifacts to process.</p> <p>API Structure:</p> <ul> <li>Group: <code>arc.bwi.de</code></li> <li>Version: <code>v1alpha1</code></li> <li>Kind: <code>Order</code></li> <li>Spec Fields:</li> <li><code>defaults</code>: Default source and destination endpoints</li> <li><code>artifacts</code>: Array of artifact specifications</li> </ul>"},{"location":"user-guide/core-concepts/#artifactworkflow","title":"ArtifactWorkflow","text":"<p>Represents a single artifact operation, generated from Order resources by the Order controller.</p> <p>Generation Logic: The OrderReconciler decomposes an Order into individual ArtifactWorkflows, applying defaults from the Order spec to each ArtifactWorkflow that doesn't specify its own <code>srcRef</code> or <code>dstRef</code>.</p>"},{"location":"user-guide/core-concepts/#endpoint","title":"Endpoint","text":"<p>Defines connection details for artifact sources and destinations.</p> <p>Spec Fields:</p> <ul> <li><code>type</code>: Endpoint type (e.g., <code>oci</code>, <code>s3</code>, <code>helm</code>)</li> <li><code>remoteURL</code>: Connection URL</li> <li><code>secretRef</code>: Reference to Secret containing credentials</li> <li><code>usage</code>: Enum (<code>PullOnly</code>, <code>PushOnly</code>, <code>All</code>)</li> </ul>"},{"location":"user-guide/core-concepts/#artifacttype","title":"ArtifactType","text":"<p>Defines processing rules and workflow templates for specific artifact types.</p> <p>Spec Fields:</p> <ul> <li><code>rules</code>: Validation rules for source and destination endpoint types</li> <li><code>defaults</code>: Default endpoint references</li> <li><code>workflowTemplateRef</code>: Reference to Argo WorkflowTemplate</li> </ul>"},{"location":"user-guide/core-concepts/#controller-architecture","title":"Controller Architecture","text":""},{"location":"user-guide/core-concepts/#order-reconciliation-flow","title":"Order Reconciliation Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant K8sAPI as \"Kubernetes API\"\n    participant ARCAPI as \"ARC API Server\"\n    participant OrderReconciler as \"OrderReconciler&lt;br/&gt;(pkg/controller)\"\n    participant etcd as \"etcd Storage\"\n    participant Argo as \"Argo Workflows\"\n\n    User-&gt;&gt;K8sAPI: Create Order resource\n    K8sAPI-&gt;&gt;ARCAPI: Forward arc.bwi.de request\n    ARCAPI-&gt;&gt;etcd: Store Order\n\n    Note over OrderReconciler: Watch loop detects change\n\n    etcd--&gt;&gt;OrderReconciler: Order event\n    OrderReconciler-&gt;&gt;OrderReconciler: Reconcile()\n\n    OrderReconciler-&gt;&gt;ARCAPI: List existing ArtifactWorkflows for Order\n    ARCAPI--&gt;&gt;OrderReconciler: ArtifactWorkflow list\n\n    OrderReconciler-&gt;&gt;OrderReconciler: Calculate desired ArtifactWorkflows&lt;br/&gt;from Order.spec.artifacts\n\n    OrderReconciler-&gt;&gt;ARCAPI: Create/Update ArtifactWorkflows\n    ARCAPI-&gt;&gt;etcd: Store ArtifactWorkflows\n\n    OrderReconciler-&gt;&gt;ARCAPI: Get ArtifactType\n    ARCAPI--&gt;&gt;OrderReconciler: ATD with workflowTemplateRef\n\n    OrderReconciler-&gt;&gt;K8sAPI: Create Workflow (via Argo API)\n    K8sAPI-&gt;&gt;Argo: Workflow created\n\n    Argo-&gt;&gt;Argo: Execute workflow\n\n    Argo-&gt;&gt;K8sAPI: Update Workflow status\n    K8sAPI--&gt;&gt;OrderReconciler: Status change event\n\n    OrderReconciler-&gt;&gt;ARCAPI: Update Order status\n    ARCAPI-&gt;&gt;etcd: Store updated status</code></pre>  Hold \"Alt\" / \"Option\" to enable pan &amp; zoom  <p>The OrderReconciler implements the controller-runtime <code>Reconciler</code> interface and is registered in the controller manager.</p> <p>Reconciliation Logic:</p> <ol> <li>Fetch Order resource</li> <li>Generate ArtifactWorkflow specifications from <code>Order.spec.artifacts</code></li> <li>Apply defaults from <code>Order.spec.defaults</code> to ArtifactWorkflows</li> <li>Create/update ArtifactWorkflow resources via ARC API Server</li> <li>Lookup ArtifactType for each ArtifactWorkflow type</li> <li>Create Argo Workflow instances using WorkflowTemplate from ATD</li> <li>Update Order status based on ArtifactWorkflow and Workflow states</li> </ol>"},{"location":"user-guide/core-concepts/#storage-architecture","title":"Storage Architecture","text":""},{"location":"user-guide/core-concepts/#dedicated-etcd-instance","title":"Dedicated etcd Instance","text":"<p>Unlike CRD-based solutions that share the cluster's etcd, ARC uses a dedicated etcd cluster. This architectural choice provides:</p> Benefit Description Isolation ARC resource storage doesn't impact cluster control plane Scalability Independent scaling for high artifact throughput Flexibility Storage backend can be replaced without API changes Performance Optimized storage parameters for ARC's access patterns <p>The API Server connects to etcd using standard etcd v3 client configuration. In the test environment, this is provided by envtest's embedded etcd:</p> <p>pkg/envtest/environment.go:56 - The API Server is configured with etcd servers from the test environment's control plane.</p>"},{"location":"user-guide/core-concepts/#workflow-integration","title":"Workflow Integration","text":""},{"location":"user-guide/core-concepts/#argo-workflows-execution-model","title":"Argo Workflows Execution Model","text":"<p>ARC delegates actual artifact processing to Argo Workflows, which provides:</p> <ul> <li>DAG-based workflow execution</li> <li>Container-native artifact handling</li> <li>Retry and failure handling</li> <li>Status reporting</li> </ul> <pre><code>graph TB\n    subgraph \"ARC Resources\"\n        ArtifactWorkflow[\"ArtifactWorkflow\"]\n        ATD[\"ArtifactType\"]\n        Endpoint1[\"Endpoint (source)\"]\n        Endpoint2[\"Endpoint (destination)\"]\n    end\n\n    subgraph \"Argo Resources\"\n        WorkflowTemplate[\"WorkflowTemplate&lt;br/&gt;(defines steps)\"]\n        Workflow[\"Workflow&lt;br/&gt;(execution instance)\"]\n    end\n\n    subgraph \"Workflow Steps\"\n        PullStep[\"Pull Artifact&lt;br/&gt;(from srcRef)\"]\n        ScanStep[\"Security Scan&lt;br/&gt;(Trivy, ClamAV)\"]\n        ValidateStep[\"Validate&lt;br/&gt;(signatures, policies)\"]\n        PushStep[\"Push Artifact&lt;br/&gt;(to dstRef)\"]\n    end\n\n    ArtifactWorkflow --&gt; ATD\n    ATD --&gt; WorkflowTemplate\n    ArtifactWorkflow --&gt; Workflow\n    WorkflowTemplate --&gt; Workflow\n\n    ArtifactWorkflow --&gt; Endpoint1\n    ArtifactWorkflow --&gt; Endpoint2\n\n    Workflow --&gt; PullStep\n    PullStep --&gt; ScanStep\n    ScanStep --&gt; ValidateStep\n    ValidateStep --&gt; PushStep\n\n    Endpoint1 -.credentials.-&gt; PullStep\n    Endpoint2 -.credentials.-&gt; PushStep</code></pre>  Hold \"Alt\" / \"Option\" to enable pan &amp; zoom  <p>Workflow Creation: The OrderReconciler creates Workflow instances by:</p> <ol> <li>Reading the <code>workflowTemplateRef</code> from the ArtifactType</li> <li>Instantiating a Workflow from the template</li> <li>Passing ArtifactWorkflow metadata and Endpoint references as workflow parameters</li> <li>Submitting the Workflow to Argo via Kubernetes API</li> </ol> <p>Status Propagation: Workflow status changes are watched by the OrderReconciler and propagated to ArtifactWorkflow and Order status fields.</p>"},{"location":"user-guide/core-concepts/#testing-infrastructure","title":"Testing Infrastructure","text":""},{"location":"user-guide/core-concepts/#test-environment-architecture","title":"Test Environment Architecture","text":"<pre><code>graph TB\n    subgraph \"Test Suite\"\n        TestCode[\"Integration Tests&lt;br/&gt;(Ginkgo)\"]\n    end\n\n    subgraph \"envtest.Environment\"\n        EnvtestCtrl[\"envtest Control Plane&lt;br/&gt;(API Server + etcd)\"]\n        EnvtestEtcd[\"envtest etcd\"]\n\n        APIServerProc[\"arc-apiserver Process&lt;br/&gt;(started via buildutils)\"]\n\n        APIService[\"APIService Resource&lt;br/&gt;(registers arc.bwi.de)\"]\n    end\n\n    subgraph \"Test Resources\"\n        TestNS[\"Test Namespace\"]\n        TestOrder[\"Test Order\"]\n        TestArtifactWorkflow[\"Test ArtifactWorkflow\"]\n    end\n\n    TestCode --&gt; EnvtestCtrl\n    TestCode --&gt; APIServerProc\n\n    EnvtestCtrl --&gt; EnvtestEtcd\n    EnvtestCtrl --&gt; APIService\n\n    APIServerProc --&gt; EnvtestEtcd\n    APIService --&gt; APIServerProc\n\n    TestCode --&gt; TestNS\n    TestCode --&gt; TestOrder\n    TestCode --&gt; TestArtifactWorkflow\n\n    TestOrder -.stored in.-&gt; EnvtestEtcd\n    TestArtifactWorkflow -.stored in.-&gt; EnvtestEtcd</code></pre>  Hold \"Alt\" / \"Option\" to enable pan &amp; zoom  <p>The test environment (<code>pkg/envtest/environment.go</code>) provides a complete ARC deployment for integration testing:</p> <p>Initialization: pkg/envtest/environment.go:30-40</p> <ul> <li>Creates envtest Kubernetes environment with APIService definitions</li> <li>Starts arc-apiserver binary via buildutils</li> <li>Configures API Server to use envtest etcd</li> <li>Waits for APIService readiness</li> </ul> <p>Test Setup: pkg/apiserver/suite_test.go:67-83</p> <ul> <li>Creates isolated test namespaces per test case</li> <li>Provides Kubernetes client configured for ARC resources</li> <li>Automatic cleanup on test completion</li> </ul> <p>Sources: pkg/envtest/environment.go:1-93, pkg/apiserver/suite_test.go:1-84</p>"},{"location":"user-guide/core-concepts/#component-interactions","title":"Component Interactions","text":""},{"location":"user-guide/core-concepts/#request-flow-for-artifact-processing","title":"Request Flow for Artifact Processing","text":"<pre><code>sequenceDiagram\n    participant K8sAPI as \"Kubernetes API\"\n    participant APIAgg as \"API Aggregation\"\n    participant ARCAPI as \"arc-apiserver\"\n    participant etcd as \"Dedicated etcd\"\n    participant Ctrl as \"OrderReconciler\"\n    participant Argo as \"Argo Controller\"\n    participant Worker as \"Workflow Pod\"\n    participant Registry as \"OCI Registry\"\n\n    CLI-&gt;&gt;K8sAPI: POST /apis/arc.bwi.de/v1alpha1/orders\n    K8sAPI-&gt;&gt;APIAgg: Route to arc.bwi.de\n    APIAgg-&gt;&gt;ARCAPI: Forward request\n    ARCAPI-&gt;&gt;etcd: Write Order\n    etcd--&gt;&gt;ARCAPI: Success\n    ARCAPI--&gt;&gt;CLI: Order created\n\n    Note over Ctrl: Watch event received\n\n    Ctrl-&gt;&gt;ARCAPI: GET Order\n    ARCAPI-&gt;&gt;etcd: Read Order\n    etcd--&gt;&gt;ARCAPI: Order data\n    ARCAPI--&gt;&gt;Ctrl: Order object\n\n    Ctrl-&gt;&gt;Ctrl: Generate ArtifactWorkflows\n\n    Ctrl-&gt;&gt;ARCAPI: POST ArtifactWorkflows\n    ARCAPI-&gt;&gt;etcd: Write ArtifactWorkflows\n\n    Ctrl-&gt;&gt;ARCAPI: GET ArtifactType\n    ARCAPI-&gt;&gt;etcd: Read ATD\n    etcd--&gt;&gt;ARCAPI: ATD with workflowTemplateRef\n    ARCAPI--&gt;&gt;Ctrl: ATD object\n\n    Ctrl-&gt;&gt;K8sAPI: POST Workflow\n    K8sAPI-&gt;&gt;Argo: Workflow created\n\n    Argo-&gt;&gt;K8sAPI: Create Workflow Pod\n    K8sAPI-&gt;&gt;Worker: Start Pod\n\n    Worker-&gt;&gt;ARCAPI: GET Endpoints\n    ARCAPI-&gt;&gt;etcd: Read Endpoints\n    etcd--&gt;&gt;ARCAPI: Endpoint configs\n    ARCAPI--&gt;&gt;Worker: Endpoint data\n\n    Worker-&gt;&gt;Registry: Pull artifact (source)\n    Registry--&gt;&gt;Worker: Artifact data\n\n    Worker-&gt;&gt;Worker: Security scan\n\n    Worker-&gt;&gt;Registry: Push artifact (destination)\n    Registry--&gt;&gt;Worker: Success\n\n    Worker-&gt;&gt;K8sAPI: Update Workflow status\n    K8sAPI--&gt;&gt;Ctrl: Status event\n\n    Ctrl-&gt;&gt;ARCAPI: PATCH Order status\n    ARCAPI-&gt;&gt;etcd: Update Order status</code></pre>  Hold \"Alt\" / \"Option\" to enable pan &amp; zoom  <p>This sequence shows the complete lifecycle from user command to artifact delivery, highlighting the separation between declarative resource management (ARC API Server) and execution (Argo Workflows).</p>"},{"location":"user-guide/core-concepts/#design-principles","title":"Design Principles","text":""},{"location":"user-guide/core-concepts/#separation-of-concerns","title":"Separation of Concerns","text":"Layer Responsibility Implementation API Layer Resource CRUD, validation, storage arc-apiserver + etcd Control Layer Reconciliation, ArtifactWorkflow generation OrderReconciler Execution Layer Artifact processing, scanning Argo Workflows Configuration Layer Endpoint definitions, type rules Endpoint, ATD resources"},{"location":"user-guide/core-concepts/#declarative-vs-imperative","title":"Declarative vs. Imperative","text":"<p>ARC follows Kubernetes conventions:</p> <ul> <li>Declarative: Users create Order resources describing desired artifacts</li> <li>Reconciliation: Controllers continuously reconcile actual state toward desired state</li> <li>Status Reporting: Status fields reflect current state and progress</li> </ul>"},{"location":"user-guide/core-concepts/#extension-api-server-benefits","title":"Extension API Server Benefits","text":"<p>The Extension API Server pattern provides:</p> <ol> <li>Future-Proof Storage: etcd can be replaced with alternative storage without changing the API</li> <li>Performance Isolation: High-volume artifact operations don't impact cluster control plane</li> <li>Custom Behavior: Full control over API semantics, validation, and storage strategies</li> <li>Standard Tooling: Compatible with kubectl, client-go, and GitOps tools</li> </ol> <p>Trade-offs:</p> <ul> <li>More complex deployment (requires APIService registration)</li> <li>Steeper learning curve for contributors</li> <li>Additional operational considerations (etcd management)</li> </ul>"},{"location":"user-guide/core-concepts/#summary","title":"Summary","text":"<p>ARC's architecture achieves Kubernetes-native artifact management through:</p> <ol> <li>Extension API Server: Provides declarative resource model with flexible storage backend</li> <li>Controller Pattern: OrderReconciler decomposes high-level Orders into executable ArtifactWorkflows</li> <li>Argo Integration: Leverages proven workflow engine for artifact processing</li> <li>Dedicated Storage: Isolated etcd prevents impact on cluster control plane</li> <li>Resource Model: Clean separation between configuration (Endpoint, ATD) and operations (Order, ArtifactWorkflow)</li> </ol>"},{"location":"user-guide/custom-resources/order/","title":"Order Resource","text":""},{"location":"user-guide/custom-resources/order/#order-resource","title":"Order Resource","text":"<p>The <code>Order</code> resource is the primary user-facing interface for requesting artifact operations. It allows users to declare multiple artifacts with shared default configurations.</p>"},{"location":"user-guide/custom-resources/order/#structure","title":"Structure","text":"<pre><code>apiVersion: arc.bwi.de/v1alpha1\nkind: Order\nmetadata:\n  name: example-order\n  namespace: default\nspec:\n  defaults:\n    srcRef:\n      name: docker-hub\n    dstRef:\n      name: internal-registry\n  artifacts:\n    - type: oci\n      spec:\n        image: library/alpine:3.18\n    - type: oci\n      dstRef:\n        name: other-registry\n      spec:\n        image: library/ubuntu:1.0\nstatus:\n  fragments:\n    \"abc123\": {name: \"example-order-abc123\"}\n    \"def456\": {name: \"example-order-def456\"}\n</code></pre>"}]}