{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"What is Artifact Conduit (ARC)?","text":"<p>Project ARC is an open-source system to bring a variety of artifact types into air-gapped environments. It acts as a gateway to get artifacts from one zone to another while scans assure that only artifacts that match certain policies are transported. These scans include malware, CVE, license scans including validation of signatures and the attestation of that process.</p>"},{"location":"#system-capabilities","title":"System Capabilities","text":"<p>ARC provides a comprehensive artifact management platform with the following key capabilities:</p>"},{"location":"#artifact-procurement","title":"Artifact Procurement","text":"<ul> <li>Multiple Source Types: OCI registries (container images, Helm charts, OCM packages), Helm repositories, S3-compatible storage, Google Cloud Storage, Azure Blob Storage, HTTP(S) endpoints</li> <li>Flexible Ordering: One-shot orders for specific versions, watchers using semver for continuous ordering, complete registry mirroring</li> </ul>"},{"location":"#endpoint-management","title":"Endpoint Management","text":"<ul> <li>Ownership Tracking: Define responsible contacts for endpoints and configurations</li> <li>Shared Endpoints: Global endpoint availability across the ARC instance</li> <li>Lifecycle Management: Endpoint expiry based on contract terms</li> <li>Policy Enforcement: Blocklist/allowlist policies for administrators</li> </ul>"},{"location":"#security-and-validation","title":"Security and Validation","text":"<ul> <li>Signature Validation: Verify artifact signatures before transport</li> <li>Malware Scanning: Plugin system supporting Trivy, Sysdig, ClamAV</li> <li>CVE Scanning: Security vulnerability detection</li> <li>License Scanning: License compliance verification</li> <li>Attestation: Sign and attest to the scanning process</li> </ul>"},{"location":"#transport-and-optimization","title":"Transport and Optimization","text":"<ul> <li>Multi-Destination: Deliver artifacts to one or more destinations after validation</li> <li>Dry-Run Mode: Test orders without actual delivery</li> <li>Deduplication: Avoid redundant transfers and scans unless necessary (e.g., CVE database updates)</li> <li>TTL Management: Automatic retirement of orders and resources</li> </ul>"},{"location":"developer-guide/contributing/","title":"Artifact Conduit (ARC) Developer Guide","text":"<p>This guide provides technical information for developers contributing to the Artifact Conduit (ARC) project. It covers the development workflow, build system, code organization, and common development tasks. For detailed information about specific topics, see the referenced sections.</p>"},{"location":"developer-guide/contributing/#development-workflow-overview","title":"Development Workflow Overview","text":"<p>ARC follows a code-generation-heavy pattern typical in Kubernetes ecosystem projects. Changes to API types trigger code regeneration, which produces client libraries, OpenAPI specifications, and CRD manifests.</p>"},{"location":"developer-guide/contributing/#build-system","title":"Build System","text":"<p>The ARC build system uses a Makefile to orchestrate various tools, designed for reproducibility. All required tools are provided in the <code>bin/</code> directory.</p> Target Purpose Key Tools Used <code>make codegen</code> Generate client-go libraries &amp; OpenAPI <code>openapi-gen</code>, <code>kube_codegen.sh</code> <code>make manifests</code> Generate CRDs and RBAC manifests <code>controller-gen</code> <code>make fmt</code> Format code, add license headers <code>addlicense</code>, <code>go fmt</code> <code>make lint</code> Run linters and checks <code>golangci-lint</code>, <code>shellcheck</code>, <code>addlicense</code> <code>make test</code> Run all tests with coverage <code>ginkgo</code>, <code>setup-envtest</code> <code>make clean</code> Remove generated binaries -"},{"location":"developer-guide/contributing/#tool-versions","title":"Tool Versions","text":"<p>The system pins specific tool versions for reproducibility:</p> <ul> <li>BDD testing framework: <code>v2.27.2</code></li> <li>Go linter: <code>v2.5.0</code></li> <li>CRD/RBAC generator: <code>v0.19.0</code></li> <li>Kubernetes test API server: <code>release-0.22</code></li> <li>K8s for integration tests: <code>1.34.1</code></li> </ul>"},{"location":"developer-guide/contributing/#codebase-organization","title":"Codebase Organization","text":"<p>ARC codebase follows standard Kubernetes project conventions:</p> Directory Purpose Generated/Manual <code>api/arc/v1alpha1/</code> Custom resource type definitions Manual <code>client-go/</code> Client libraries for ARC resources Generated <code>pkg/apiserver/</code> Extension API server implementation Manual <code>pkg/controller/</code> Controller reconciliation logic Manual <code>pkg/registry/</code> Storage strategies for custom resources Manual <code>cmd/arcctl/</code> CLI tool implementation Manual <code>config/</code> Kubernetes manifests (CRDs, RBAC) Generated <code>hack/</code> Build and code generation scripts Manual"},{"location":"developer-guide/contributing/#code-generation-process","title":"Code Generation Process","text":"<p>ARC uses the Kubernetes code-generator to produce client libraries and OpenAPI specs.</p> <ul> <li><code>make codegen</code> triggers <code>hack/update-codegen.sh</code></li> <li>Generates:</li> <li>Client-go libraries in <code>client-go/</code></li> <li>OpenAPI specs</li> <li>CRD manifests</li> </ul> <p>See Client Libraries section for usage details.</p>"},{"location":"developer-guide/contributing/#testing-strategy","title":"Testing Strategy","text":"<p>ARC uses a multi-layered testing strategy:</p> <ul> <li>Unit Tests</li> <li>Integration Tests (uses <code>ENVTEST_K8S_VERSION=1.34.1</code>)</li> <li>Controller Tests via envtest</li> </ul> <p>Run all tests and generate coverage:</p> <pre><code>make test\n</code></pre> <p>Setup environment for integration tests:</p> <pre><code>setup-envtest\nexport ENVTEST_K8S_VERSION=1.34.1\n</code></pre> <p>Test coverage is tracked using Coveralls.</p>"},{"location":"developer-guide/contributing/#continuous-integration-ci-pipeline","title":"Continuous Integration (CI) Pipeline","text":"<p>Pipeline runs on every push and pull request, enforcing code quality and test coverage.</p> <ul> <li>Lint Job</li> <li><code>addlicense</code></li> <li><code>shellcheck</code></li> <li><code>golangci-lint</code></li> <li>Test Job (runs after Lint)</li> <li><code>make test</code></li> </ul> <p>For customization details, see <code>.github/workflows/golang.yaml</code>.</p>"},{"location":"developer-guide/contributing/#adding-a-new-custom-resource","title":"Adding a New Custom Resource","text":"<p>To introduce a new CRD:</p> <ol> <li>Create type definition in <code>api/arc/v1alpha1/</code></li> <li>Add OpenAPI model name</li> <li>Regenerate code via <code>make codegen</code></li> <li>Implement storage in <code>pkg/registry/</code></li> <li>Add controller logic in <code>pkg/controller/</code> if reconciliation is needed</li> </ol> <p>See <code>hack/update-codegen.sh</code> for implementation details.</p>"},{"location":"developer-guide/contributing/#modifying-existing-api-types","title":"Modifying Existing API Types","text":"<p>Typical steps:</p> <ol> <li>Edit types in <code>api/arc/v1alpha1/</code></li> <li>Run <code>make codegen</code></li> <li>Run <code>make manifests</code></li> <li>Run <code>make test</code></li> </ol> <p>Note: Breaking changes may affect existing clients. Follow semantic versioning and provide migration paths.</p>"},{"location":"developer-guide/contributing/#code-quality-linting","title":"Code Quality &amp; Linting","text":"<p>Lint and license checks before committing:</p> <ul> <li><code>addlicense</code> for Apache 2.0 headers</li> <li><code>shellcheck</code> for scripts in <code>hack/</code></li> <li><code>golangci-lint</code> for Go linting</li> </ul> <p>Fix issues with:</p> <pre><code>make fmt\nmake lint\n</code></pre>"},{"location":"developer-guide/contributing/#environment-variables","title":"Environment Variables","text":"Variable Purpose Default <code>BUILD_PATH</code> Build output directory <code>$(pwd)</code> <code>LOCALBIN</code> Tool install directory <code>$(BUILD_PATH)/bin</code> <code>GOPRIVATE</code> Private Go modules <code>*.opencode.de</code> <code>GNOSUMDB</code> Disable checksum DB <code>*.opencode.de</code> <code>KUBEBUILDER_ASSETS</code> envtest binaries Set by <code>setup-envtest</code> <p>For deeper information on development topics, consult the corresponding sections and scripts.</p>"},{"location":"developer-guide/dod/","title":"Definition of Done","text":"<p>TODO</p>"},{"location":"developer-guide/working-with-docs/","title":"Documentation Setup","text":"<p>The documentation of the ARC project is written primarily using Markdown. All documentation related content can be found in https://github.com/opendefensecloud/artifact-conduit/tree/main/docs. New content also should be added there.</p> <p>To render the documentation with <code>mkdocs</code> locally use <code>make docs</code> and open the local page:</p> <pre><code>$ make docs\nmkdocs serve\nINFO    -  Building documentation...\nINFO    -  Cleaning site directory\nINFO    -  Documentation built in 0.22 seconds\nINFO    -  [13:29:25] Watching paths for changes: 'docs', 'mkdocs.yml'\nINFO    -  [13:29:25] Serving on http://127.0.0.1:8000/\n</code></pre>"},{"location":"developer-guide/adrs/000-Use-Markdown-Architectural-Decision-Records/","title":"000 Use Markdown Architectural Decision Records","text":""},{"location":"developer-guide/adrs/000-Use-Markdown-Architectural-Decision-Records/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We want to record architectural decisions made in this project independent whether decisions concern the architecture (\"architectural decision record\"), the code, or other fields. Which format and structure should these records follow?</p>"},{"location":"developer-guide/adrs/000-Use-Markdown-Architectural-Decision-Records/#considered-options","title":"Considered Options","text":"<ul> <li>MADR\u00a04.0.0 \u2013 The Markdown Architectural Decision Records</li> <li>Michael Nygard's template\u00a0\u2013 The first incarnation of the term \"ADR\"</li> <li>Sustainable Architectural Decisions\u00a0\u2013 The Y-Statements</li> <li>Other templates listed at\u00a0https://github.com/joelparkerhenderson/architecture_decision_record</li> <li>Formless \u2013 No conventions for file format and structure</li> </ul>"},{"location":"developer-guide/adrs/000-Use-Markdown-Architectural-Decision-Records/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"MADR 4.0.0\", because</p> <ul> <li>Implicit assumptions should be made explicit. Design documentation is important to enable people understanding the decisions later on. See also\u00a0\"A rational design process: How and why to fake it\".</li> <li>MADR allows for structured capturing of any decision.</li> <li>The MADR format is lean and fits our development style.</li> <li>The MADR structure is comprehensible and facilitates usage &amp; maintenance.</li> <li>The MADR project is vivid.</li> </ul>"},{"location":"developer-guide/adrs/001-ARC-Architecture/","title":"001 ARC Architecture","text":""},{"location":"developer-guide/adrs/001-ARC-Architecture/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>This ADR is about finding the right architecture for the ARC suite of services based on the knowledge gained during the internal predecessors of this project.</p>"},{"location":"developer-guide/adrs/001-ARC-Architecture/#glossary","title":"Glossary","text":"<ul> <li><code>arcctl</code>: Command line utility to interact with the ARC API.</li> <li><code>Order</code>: Represents an order of one or more artifacts. Even ordering artifacts that may exist in the future can be reference here using semver expressions for example.</li> <li><code>OrderFragment</code>: Represents a single artifact order which is part of an <code>Order</code>.</li> <li><code>OrderTypeDefinition</code>: Defines rules and defaults for a specific order type like 'OCI'. References a certain workflow to use for that type.</li> <li><code>Endpoint</code>: General term for source or destination. Can be a source or destination for artifacts. Includes optional credentials to access it.</li> <li><code>WorkflowTemplate</code>: Argo Workflows, see https://argo-workflows.readthedocs.io/en/latest/fields/#workflowtemplate</li> <li><code>Workflow</code>: Argo Workflows, see https://argo-workflows.readthedocs.io/en/latest/fields/#workflow</li> <li><code>ARC API Server</code>: A Kubernetes Extension API Server which handles storage of ARC API</li> <li><code>Order Controller</code>: A Kubernetes Controller which reconciles <code>Orders</code>, splits up <code>Order</code> resources into <code>OrderFragment</code> Resources, creates <code>Workflow</code> resources for necessary workload</li> <li><code>ArtifactTypeDefinition</code>: Specifies the processing rules and workflow templates for artifact types (e.g. <code>oci</code>, <code>helm</code>).</li> </ul>"},{"location":"developer-guide/adrs/001-ARC-Architecture/#considered-options","title":"Considered Options","text":""},{"location":"developer-guide/adrs/001-ARC-Architecture/#classic-kubernetes-operators","title":"Classic Kubernetes Operators","text":"<ul> <li>CRDs are used to interact with ARC via the Kubernetes API Server.</li> <li>Several operators come into play which reconcile the different custom resources.</li> <li>A sharding mechanism is implemented to be able to scale the workers horizontally and give every worker a given chunk of resources to reconcile.</li> </ul>"},{"location":"developer-guide/adrs/001-ARC-Architecture/#pros","title":"Pros","text":"<ul> <li>CRDs and Kubernetes are relatively simple to implement</li> </ul>"},{"location":"developer-guide/adrs/001-ARC-Architecture/#cons","title":"Cons","text":"<ul> <li>Storage may be limited by <code>etcd</code> and can bring the control plane of Kubernetes into trouble if too many resources are present</li> <li>The necessity to implement sharding may be hard work and error prone</li> <li>Thus said it may not scale in way necessary for such a solution</li> </ul>"},{"location":"developer-guide/adrs/001-ARC-Architecture/#extension-api-server-and-cncf-landscape-tooling","title":"Extension API Server and CNCF Landscape Tooling","text":""},{"location":"developer-guide/adrs/001-ARC-Architecture/#flavor-a","title":"Flavor A","text":"<p>Flavor uses several controllers to handle different parts of the API. Orders are reconciled and converted to hydrated Orders which contain the source and destination information along with the artifact to process. This information are published to some message queue which is subscribed by workers working on that queue. Optionally KEDA is used to scale, handle quotas and fairness. The database to be used for the API Server is etcd.</p>"},{"location":"developer-guide/adrs/001-ARC-Architecture/#flavor-b","title":"Flavor B","text":"<p>Same as Flavor A except the database for the API Server is something like Postgres instead of etcd. Additionally the option is considered to let the controller access the Postgres directly to reconcile without using the Kubernetes API Server.</p>"},{"location":"developer-guide/adrs/001-ARC-Architecture/#flavor-c","title":"Flavor C","text":"<p>Same as Flavor B except that no message queue is used but the job queue is stored directly in the Postgres database.</p>"},{"location":"developer-guide/adrs/001-ARC-Architecture/#flavor-d","title":"Flavor D","text":"<p>Same as Flavor A except that no message queue is used. The Order controller creates hydrated orders directly in <code>etcd</code> as readonly resource which is then consumed by workers directly. This approach needs some kind of sharding mechanism to have the workers to know which shard of resources they need to handle.</p>"},{"location":"developer-guide/adrs/001-ARC-Architecture/#flavor-e","title":"Flavor E","text":"<p>This flavor is the most compelling due to the reduced amount of code that is necessary to bring this solution to live. etcd is used as storage for the API server. Argo Workflows is used to build workflows which do the steps necessary to process one artifact. Kueue can be used to bring fairness, scaling, quotas into play which workflows. The order controller creates \"jobs\" which are actually Argo Workflows.</p> <p>This option is described in detail in the following document.</p>"},{"location":"developer-guide/adrs/001-ARC-Architecture/#technology","title":"Technology","text":"<ul> <li>Instead of CRDs, <code>ARC</code> uses an Extension API Server via the Kubernetes API Aggregation Layer to handle API requests.</li> <li>This gives it the possibility to use a dedicated <code>etcd</code> or a even more suitable storage backend for the high amount of resources and status information in case this is necessary.</li> <li>While <code>etcd</code> still can be used as storage backend, it is one separated from the <code>etcd</code> used by the Kubernetes control plane and reduces the risk of bringing the whole cluster into trouble.</li> <li>Additional links</li> <li>https://github.com/kubernetes-sigs/apiserver-runtime</li> <li>https://github.com/kubernetes/sample-apiserver/tree/master</li> <li>Utilize Argo Workflows to handle the workflows necessary to process different artifact types</li> <li>Optionally use Kueue to handle quotas and enhanced scheduling</li> <li>Namespaces are used to separate resources in a multi-tenant environment.</li> </ul>"},{"location":"developer-guide/adrs/001-ARC-Architecture/#architecture-diagram","title":"Architecture Diagram","text":"<p>Overview Diagram</p> <p></p> <p>API Concept Diagram</p> <pre><code>---\nconfig:\n  layout: elk\n---\nflowchart LR\n subgraph subGraph0[\"Declarative Layer\"]\n        Order[\"Order (User Input)\"]\n        Spec[\"spec:&lt;br&gt;- defaults&lt;br&gt;- artifacts[]\"]\n  end\n subgraph subGraph1[\"Generated Layer\"]\n        Fragment1[\"Fragment-1\"]\n        Fragment2[\"Fragment-2\"]\n        FragmentN[\"Fragment-N\"]\n  end\n subgraph Configuration[\"Configuration\"]\n        ArtifactTypeDef@{ label: \"ArtifactTypeDefinition (e.g., 'oci')\" }\n        EndpointSrc[\"Endpoint (Source)\"]\n        EndpointDst[\"Endpoint (Destination)\"]\n        Secret[\"Secret (Credentials)\"]\n  end\n subgraph Execution[\"Execution\"]\n        WorkflowTemplate[\"WorkflowTemplate (Argo)\"]\n        WorkflowInstance1[\"Workflow Instance\"]\n        WorkflowInstance2[\"Workflow Instance\"]\n  end\n    Order -- contains --&gt; Spec\n    Spec -- generates --&gt; Fragment1 &amp; Fragment2 &amp; FragmentN\n    Fragment1 -- type --&gt; ArtifactTypeDef\n    Fragment2 -- type --&gt; ArtifactTypeDef\n    Fragment1 -- srcRef --&gt; EndpointSrc\n    Fragment2 -- srcRef --&gt; EndpointSrc\n    Fragment1 -- dstRef --&gt; EndpointDst\n    Fragment2 -- dstRef --&gt; EndpointDst\n    Fragment1 -- references --&gt; EndpointSrc &amp; EndpointDst\n    Fragment2 -- references --&gt; EndpointSrc &amp; EndpointDst\n    EndpointSrc -- credentialRef --&gt; Secret\n    EndpointDst -- credentialRef --&gt; Secret\n    ArtifactTypeDef -- workflowTemplateRef --&gt; WorkflowTemplate\n    Fragment1 -- triggers --&gt; WorkflowTemplate\n    Fragment2 -- triggers --&gt; WorkflowTemplate\n    WorkflowTemplate -- instantiates --&gt; WorkflowInstance1 &amp; WorkflowInstance2\n    ArtifactTypeDef@{ shape: rect}\n</code></pre> <p>The solution shows the ARC API Server which handles storage for the custom resources / API of ARC. <code>etcd</code> is used as storage solution. <code>Order Controller</code> is a classic Kubernetes controller implementation which reconciles <code>Orders</code> and <code>Endpoints</code>. An <code>Order</code> contains the information what artifacts should be processed. An <code>Endpoint</code> contains the information about a source or destination for artifacts. The <code>Order Controller</code> creates <code>Fragment</code> resources which are single artifacts decomposed from an <code>Order</code>. An <code>ArtifactTypeDefinition</code> specifies the processing rules and workflow templates for artifact types (e.g. <code>oci</code>, <code>helm</code>).</p>"},{"location":"developer-guide/adrs/001-ARC-Architecture/#pros_1","title":"Pros","text":"<ul> <li>Using dedicated <code>etcd</code> does not clutter the infra etcd</li> <li>Storage can be changed later on if necessary</li> <li>Keep the declarative style of Kubernetes while having complete freedom on the API implementation</li> <li>Argo Workflows allows us to focus on the domain of the product without reinventing the wheel</li> <li>Qotas and Fairness easy without writing code via Kueue</li> </ul>"},{"location":"developer-guide/adrs/001-ARC-Architecture/#cons_1","title":"Cons","text":"<ul> <li>Building addon apiservers directly on the raw api-machinery libraries requires non-trivial code that must be maintained and rebased as the raw libraries change.</li> <li>Steep learning curve when starting the project and steeper learning curve when joining the project.</li> </ul>"},{"location":"developer-guide/adrs/001-ARC-Architecture/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen Option: Solution A.</p> <p>Because the solution is the one that provides the most flexibility while the necessity to write own code for many parts is minimized. The flexibility comes from utilizing the CNCF projects Argo Workflows and Kueue for building the workflow engine. The project itself can focus on the order process and the handling of endpoints.</p>"},{"location":"developer-guide/adrs/002-ARC-API/","title":"002 ARC API","text":""},{"location":"developer-guide/adrs/002-ARC-API/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>This ADR is about finding the right API for ARC.</p>"},{"location":"developer-guide/adrs/002-ARC-API/#proposed-solution","title":"Proposed Solution","text":"<p>Options were discussed and documented here: https://app.bwi.conceptboard.com/board/u9c0-4nk5-rrhd-knre-6cfn</p> <pre><code>apiVersion: arc.bwi.de/v1alpha1\nkind: Order\nmetadata:\n  name: example-order\nspec:\n  defaults:\n    srcRef:\n      name: docker-hub\n      namespace: default # optional\n    dstRef:\n      name: internal-registry\n  artifacts:\n    - type: oci # artifactType, correcesponds to workflow\n      dstRef:\n        name: other-internal-registry\n        namespace: default # optional\n      spec:\n        image: library/alpine:3.18\n        override: myteam/alpine:3.18-dev # default alpine:3.18; support CEL?\n    - type: oci\n      spec:\n        image: library/ubuntu:1.0\n    - type: helm\n      srcRef:\n        name: jetstack-helm\n      dstRef:\n        name: internal-helm-registry\n      spec:\n        name: cert-manager\n        version: \"47.11\"\n        override: helm-charts/cert-manager:47.11\n</code></pre> <pre><code>apiVersion: arc.bwi.de/v1alpha1\nkind: Fragment\nmetadata:\n  name: example-order-1 # sha256 for procedural\nspec:\n  type: oci # artifactType, correcesponds to workflow\n  srcRef: # required\n    name: lala\n  dstRef: #required\n    name: other-internal-registry\n    namespace: default # optional\n  spec:\n    image: library/alpine:3.18\n    override: myteam/alpine:3.18-dev # default alpine:3.18; support CEL?\n</code></pre> <pre><code>apiVersion: arc.bwi.de/v1alpha1\nkind: Endpoint\nmetadata:\n  name: internal-registry\nspec:\n  type: oci # Endpoint Type! set valid types on controller manager?\n  remoteURL: https://artifactory.example.com/artifactory/ace-oci-local\n  secretRef: # STANDARDIZED!\n    name: internal-registry-credentials\n  usage: PullOnly | PushOnly | All # enum\n</code></pre> <pre><code>apiVersion: arc.bwi.de/v1alpha1\nkind: ArtifactTypeDefinition\nmetadata:\n  name: oci\nspec:\n  rules:\n    srcTypes:\n    - s3 # Endpoint Types!\n    - oci\n    - helm\n    dstTypes:\n    - oci\n  defaults:\n    dstRef: internal-registry\n  workflowTemplateRef: # argo.Workflow\n</code></pre>"},{"location":"operator-manual/installation/","title":"Installation","text":"<p>TODO</p>"},{"location":"operator-manual/new-features/","title":"New Features","text":"<p>TODO</p>"},{"location":"operator-manual/releases/","title":"Releases","text":"<p>TODO</p>"},{"location":"operator-manual/security/","title":"Security","text":"<p>TODO</p>"},{"location":"operator-manual/upgrading/","title":"Upgrading","text":"<p>TODO</p>"},{"location":"user-guide/core-concepts/","title":"Core Concepts","text":"<p>ARC is built as a Kubernetes-native system that extends the Kubernetes API through the API Aggregation Layer. Unlike traditional Kubernetes operators that use Custom Resource Definitions (CRDs), ARC implements an Extension API Server with dedicated storage. This architectural decision provides isolation from the cluster's control plane etcd and enables future flexibility in storage backend selection.</p> <p>The system consists of three primary runtime components:</p> <ul> <li>ARC API Server - Extends the Kubernetes API to handle arc.bwi.de/v1alpha1 resources</li> <li>Order Controller - Reconciles high-level artifact requests into executable units</li> <li>Argo Workflows - Executes artifact processing workflows defined by <code>ArtifactTypeDefinition</code> resources</li> </ul>"},{"location":"user-guide/core-concepts/#resource-type-hierarchy","title":"Resource Type Hierarchy","text":"Resource Type API Group Purpose Lifecycle Owner Order arc.bwi.de/v1alpha1 High-level artifact request containing multiple artifacts User/GitOps Fragment arc.bwi.de/v1alpha1 Single artifact operation unit, child of Order Order Controller Endpoint arc.bwi.de/v1alpha1 Configuration for artifact source/destination User/Admin ArtifactTypeDefinition arc.bwi.de/v1alpha1 Type rules and workflow reference for artifact processing Admin <p>The resource hierarchy establishes a declarative model where users create <code>Order</code> resources that reference shared <code>Endpoint</code> configurations. The Order Controller decomposes each Order into individual <code>Fragment</code> resources based on the <code>ArtifactTypeDefinition</code> rules, which then trigger corresponding Argo Workflows.</p>"},{"location":"user-guide/core-concepts/#order-reconciliation-sequence","title":"Order Reconciliation Sequence","text":"<p>The <code>OrderController</code> watches for <code>Order</code> resources and implements the reconciliation logic. When an <code>Order</code> is created, the controller:</p> <ol> <li>Retrieves the <code>Order</code> and referenced <code>Endpoint</code> resources</li> <li>Looks up the appropriate <code>ArtifactTypeDefinition</code> for each artifact</li> <li>Creates individual <code>Fragment</code> resources</li> <li>Creates Argo Workflow instances based on the <code>workflowTemplateRef</code></li> </ol>"},{"location":"user-guide/custom-resources/order/","title":"Order Resource","text":"<p>The <code>Order</code> resource is the primary user-facing interface for requesting artifact operations. It allows users to declare multiple artifacts with shared default configurations.</p>"},{"location":"user-guide/custom-resources/order/#structure","title":"Structure","text":"<pre><code>apiVersion: arc.bwi.de/v1alpha1\nkind: Order\nmetadata:\n  name: example-order\n  namespace: default\nspec:\n  defaults:\n    srcRef:\n      name: docker-hub\n    dstRef:\n      name: internal-registry\n  artifacts:\n    - type: oci\n      spec:\n        image: library/alpine:3.18\n    - type: oci\n      dstRef:\n        name: other-registry\n      spec:\n        image: library/ubuntu:1.0\nstatus:\n  fragments:\n    \"abc123\": {name: \"example-order-abc123\"}\n    \"def456\": {name: \"example-order-def456\"}\n</code></pre>"}]}